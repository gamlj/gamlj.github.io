<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The General Linear Model | GAMLj Models</title>
  <meta name="description" content="Examples of using GAMLj jamovi module to estimates different types of linear models" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The General Linear Model | GAMLj Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Examples of using GAMLj jamovi module to estimates different types of linear models" />
  <meta name="github-repo" content="mcfanda/jmvScaffold" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The General Linear Model | GAMLj Models" />
  
  <meta name="twitter:description" content="Examples of using GAMLj jamovi module to estimates different types of linear models" />
  

<meta name="author" content="Marcello Gallucci" />


<meta name="date" content="2023-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="booklet.html"/>
<link rel="next" href="gzlm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="mcstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GAMLj Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="booklet.html"><a href="booklet.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="booklet.html"><a href="booklet.html#preface"><i class="fa fa-check"></i><b>1.1</b> Preface</a></li>
<li class="chapter" data-level="1.2" data-path="booklet.html"><a href="booklet.html#getstarted"><i class="fa fa-check"></i><b>1.2</b> Getting Started</a></li>
<li class="chapter" data-level="1.3" data-path="booklet.html"><a href="booklet.html#data"><i class="fa fa-check"></i><b>1.3</b> Data</a></li>
<li class="chapter" data-level="1.4" data-path="booklet.html"><a href="booklet.html#naming"><i class="fa fa-check"></i><b>1.4</b> What’s in a name</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="booklet.html"><a href="booklet.html#angles"><i class="fa fa-check"></i><b>1.4.1</b> Statistical techniques vs points of view</a></li>
<li class="chapter" data-level="1.4.2" data-path="booklet.html"><a href="booklet.html#statistical-techniques-vs-analyses"><i class="fa fa-check"></i><b>1.4.2</b> Statistical techniques vs Analyses</a></li>
<li class="chapter" data-level="1.4.3" data-path="booklet.html"><a href="booklet.html#terms-that-we-need-to-generalize"><i class="fa fa-check"></i><b>1.4.3</b> Terms that we need to generalize</a></li>
<li class="chapter" data-level="1.4.4" data-path="booklet.html"><a href="booklet.html#covnames"><i class="fa fa-check"></i><b>1.4.4</b> Terms we need to cope with</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="booklet.html"><a href="booklet.html#general-references"><i class="fa fa-check"></i><b>1.5</b> General References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>2</b> The General Linear Model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="glm.html"><a href="glm.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="glm.html"><a href="glm.html#regression"><i class="fa fa-check"></i><b>2.2</b> One continuous IV</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="glm.html"><a href="glm.html#glminput"><i class="fa fa-check"></i><b>2.2.1</b> Input</a></li>
<li class="chapter" data-level="2.2.2" data-path="glm.html"><a href="glm.html#model-recap"><i class="fa fa-check"></i><b>2.2.2</b> Model Recap</a></li>
<li class="chapter" data-level="2.2.3" data-path="glm.html"><a href="glm.html#twofit"><i class="fa fa-check"></i><b>2.2.3</b> Model Fit</a></li>
<li class="chapter" data-level="2.2.4" data-path="glm.html"><a href="glm.html#omnibus-tests"><i class="fa fa-check"></i><b>2.2.4</b> Omnibus Tests</a></li>
<li class="chapter" data-level="2.2.5" data-path="glm.html"><a href="glm.html#glmcoefs"><i class="fa fa-check"></i><b>2.2.5</b> Coefficients</a></li>
<li class="chapter" data-level="2.2.6" data-path="glm.html"><a href="glm.html#pearson-correlation"><i class="fa fa-check"></i><b>2.2.6</b> Pearson Correlation</a></li>
<li class="chapter" data-level="2.2.7" data-path="glm.html"><a href="glm.html#glmplot"><i class="fa fa-check"></i><b>2.2.7</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="glm.html"><a href="glm.html#multiregression"><i class="fa fa-check"></i><b>2.3</b> More continuous IVs</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="glm.html"><a href="glm.html#twofit2"><i class="fa fa-check"></i><b>2.3.1</b> Model Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="glm.html"><a href="glm.html#variances"><i class="fa fa-check"></i><b>2.3.2</b> Omnibus Tests</a></li>
<li class="chapter" data-level="2.3.3" data-path="glm.html"><a href="glm.html#household-chores"><i class="fa fa-check"></i><b>2.3.3</b> Household Chores</a></li>
<li class="chapter" data-level="2.3.4" data-path="glm.html"><a href="glm.html#coefficients"><i class="fa fa-check"></i><b>2.3.4</b> Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="glm.html"><a href="glm.html#moderation"><i class="fa fa-check"></i><b>2.4</b> Continuous IVs and interaction</a></li>
<li class="chapter" data-level="2.5" data-path="glm.html"><a href="glm.html#modint"><i class="fa fa-check"></i><b>2.5</b> Moderation=interaction</a></li>
<li class="chapter" data-level="2.6" data-path="glm.html"><a href="glm.html#simpleslopes"><i class="fa fa-check"></i><b>2.6</b> Simple Slopes</a></li>
<li class="chapter" data-level="2.7" data-path="glm.html"><a href="glm.html#anova"><i class="fa fa-check"></i><b>2.7</b> Categorical IVs</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="glm.html"><a href="glm.html#model-fit-and-omnibus-tests"><i class="fa fa-check"></i><b>2.7.1</b> Model Fit and Omnibus tests</a></li>
<li class="chapter" data-level="2.7.2" data-path="glm.html"><a href="glm.html#dummies"><i class="fa fa-check"></i><b>2.7.2</b> Coefficients</a></li>
<li class="chapter" data-level="2.7.3" data-path="glm.html"><a href="glm.html#plots"><i class="fa fa-check"></i><b>2.7.3</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="glm.html"><a href="glm.html#posthoc"><i class="fa fa-check"></i><b>2.8</b> Post-hoc tests</a></li>
<li class="chapter" data-level="2.9" data-path="glm.html"><a href="glm.html#cohens-d"><i class="fa fa-check"></i><b>2.9</b> Cohen’s d</a></li>
<li class="chapter" data-level="2.10" data-path="glm.html"><a href="glm.html#estimated-marginal-means"><i class="fa fa-check"></i><b>2.10</b> Estimated marginal means</a></li>
<li class="chapter" data-level="2.11" data-path="glm.html"><a href="glm.html#ancova"><i class="fa fa-check"></i><b>2.11</b> Categorical and Continuous IVs</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="glm.html"><a href="glm.html#results-tables"><i class="fa fa-check"></i><b>2.11.1</b> Results tables</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="glm.html"><a href="glm.html#moderation2"><i class="fa fa-check"></i><b>2.12</b> Categorical and Continuous Interactions</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="glm.html"><a href="glm.html#plot"><i class="fa fa-check"></i><b>2.12.1</b> Plot</a></li>
<li class="chapter" data-level="2.12.2" data-path="glm.html"><a href="glm.html#simple-effects"><i class="fa fa-check"></i><b>2.12.2</b> Simple Effects</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="glm.html"><a href="glm.html#simpleinteractions"><i class="fa fa-check"></i><b>2.13</b> Simple Interactions</a></li>
<li class="chapter" data-level="2.14" data-path="glm.html"><a href="glm.html#model-comparison-approach"><i class="fa fa-check"></i><b>2.14</b> Model-comparison approach</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="glm.html"><a href="glm.html#the-method"><i class="fa fa-check"></i><b>2.14.1</b> The Method</a></li>
<li class="chapter" data-level="2.14.2" data-path="glm.html"><a href="glm.html#types-of-tests"><i class="fa fa-check"></i><b>2.14.2</b> Types of tests</a></li>
<li class="chapter" data-level="2.14.3" data-path="glm.html"><a href="glm.html#not-necessary-model-comparisons"><i class="fa fa-check"></i><b>2.14.3</b> Not necessary model-comparisons</a></li>
<li class="chapter" data-level="2.14.4" data-path="glm.html"><a href="glm.html#hierarchical-regression"><i class="fa fa-check"></i><b>2.14.4</b> Hierarchical regression</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="glm.html"><a href="glm.html#assumptions-checks"><i class="fa fa-check"></i><b>2.15</b> Assumptions checks</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="glm.html"><a href="glm.html#homosched"><i class="fa fa-check"></i><b>2.15.1</b> Homoschedasticity</a></li>
<li class="chapter" data-level="2.15.2" data-path="glm.html"><a href="glm.html#normality-of-residuals"><i class="fa fa-check"></i><b>2.15.2</b> Normality of residuals</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="glm.html"><a href="glm.html#violations-remedies"><i class="fa fa-check"></i><b>2.16</b> Violations Remedies</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="glm.html"><a href="glm.html#robust-standard-error"><i class="fa fa-check"></i><b>2.16.1</b> Robust Standard Error</a></li>
<li class="chapter" data-level="2.16.2" data-path="glm.html"><a href="glm.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>2.16.2</b> Bootstrap Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="gzlm.html"><a href="gzlm.html"><i class="fa fa-check"></i><b>3</b> The Generalized Linear Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="gzlm.html"><a href="gzlm.html#gzlmintro"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="gzlm.html"><a href="gzlm.html#logistic"><i class="fa fa-check"></i><b>3.2</b> Logistic Model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="gzlm.html"><a href="gzlm.html#the-rationale"><i class="fa fa-check"></i><b>3.2.1</b> The rationale</a></li>
<li class="chapter" data-level="3.2.2" data-path="gzlm.html"><a href="gzlm.html#model-estimation"><i class="fa fa-check"></i><b>3.2.2</b> Model Estimation</a></li>
<li class="chapter" data-level="3.2.3" data-path="gzlm.html"><a href="gzlm.html#model-recap-1"><i class="fa fa-check"></i><b>3.2.3</b> Model Recap</a></li>
<li class="chapter" data-level="3.2.4" data-path="gzlm.html"><a href="gzlm.html#model-fit"><i class="fa fa-check"></i><b>3.2.4</b> Model Fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="gzlm.html"><a href="gzlm.html#omnibus-tests-1"><i class="fa fa-check"></i><b>3.2.5</b> Omnibus Tests</a></li>
<li class="chapter" data-level="3.2.6" data-path="gzlm.html"><a href="gzlm.html#coefficients-1"><i class="fa fa-check"></i><b>3.2.6</b> Coefficients</a></li>
<li class="chapter" data-level="3.2.7" data-path="gzlm.html"><a href="gzlm.html#odd-ratios-expb"><i class="fa fa-check"></i><b>3.2.7</b> Odd ratios exp(B)</a></li>
<li class="chapter" data-level="3.2.8" data-path="gzlm.html"><a href="gzlm.html#logisticcontmarginal"><i class="fa fa-check"></i><b>3.2.8</b> Marginal effects</a></li>
<li class="chapter" data-level="3.2.9" data-path="gzlm.html"><a href="gzlm.html#multiple-ivs"><i class="fa fa-check"></i><b>3.2.9</b> Multiple IVs</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="gzlm.html"><a href="gzlm.html#gzlmanova"><i class="fa fa-check"></i><b>3.3</b> Logistic with Catecorical IVs</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="gzlm.html"><a href="gzlm.html#model-estimation-1"><i class="fa fa-check"></i><b>3.3.1</b> Model Estimation</a></li>
<li class="chapter" data-level="3.3.2" data-path="gzlm.html"><a href="gzlm.html#model-fit-1"><i class="fa fa-check"></i><b>3.3.2</b> Model Fit</a></li>
<li class="chapter" data-level="3.3.3" data-path="gzlm.html"><a href="gzlm.html#omnibus-tests-2"><i class="fa fa-check"></i><b>3.3.3</b> Omnibus Tests</a></li>
<li class="chapter" data-level="3.3.4" data-path="gzlm.html"><a href="gzlm.html#plots-1"><i class="fa fa-check"></i><b>3.3.4</b> Plots</a></li>
<li class="chapter" data-level="3.3.5" data-path="gzlm.html"><a href="gzlm.html#estimated-marginal-means-1"><i class="fa fa-check"></i><b>3.3.5</b> Estimated Marginal Means</a></li>
<li class="chapter" data-level="3.3.6" data-path="gzlm.html"><a href="gzlm.html#relative-risk"><i class="fa fa-check"></i><b>3.3.6</b> Relative Risk</a></li>
<li class="chapter" data-level="3.3.7" data-path="gzlm.html"><a href="gzlm.html#logisticcatmarginal"><i class="fa fa-check"></i><b>3.3.7</b> Marginal Effects</a></li>
<li class="chapter" data-level="3.3.8" data-path="gzlm.html"><a href="gzlm.html#post-hoc-tests"><i class="fa fa-check"></i><b>3.3.8</b> Post-hoc tests</a></li>
<li class="chapter" data-level="3.3.9" data-path="gzlm.html"><a href="gzlm.html#other-options"><i class="fa fa-check"></i><b>3.3.9</b> Other options</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="gzlm.html"><a href="gzlm.html#probit"><i class="fa fa-check"></i><b>3.4</b> Probit Model</a></li>
<li class="chapter" data-level="3.5" data-path="gzlm.html"><a href="gzlm.html#multinomial"><i class="fa fa-check"></i><b>3.5</b> The Multinomial Model</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="gzlm.html"><a href="gzlm.html#rationale"><i class="fa fa-check"></i><b>3.5.1</b> Rationale</a></li>
<li class="chapter" data-level="3.5.2" data-path="gzlm.html"><a href="gzlm.html#model-estimation-2"><i class="fa fa-check"></i><b>3.5.2</b> Model Estimation</a></li>
<li class="chapter" data-level="3.5.3" data-path="gzlm.html"><a href="gzlm.html#model-recap-2"><i class="fa fa-check"></i><b>3.5.3</b> Model Recap</a></li>
<li class="chapter" data-level="3.5.4" data-path="gzlm.html"><a href="gzlm.html#overal-fit"><i class="fa fa-check"></i><b>3.5.4</b> Overal Fit</a></li>
<li class="chapter" data-level="3.5.5" data-path="gzlm.html"><a href="gzlm.html#plots-2"><i class="fa fa-check"></i><b>3.5.5</b> Plots</a></li>
<li class="chapter" data-level="3.5.6" data-path="gzlm.html"><a href="gzlm.html#coefficients-2"><i class="fa fa-check"></i><b>3.5.6</b> Coefficients</a></li>
<li class="chapter" data-level="3.5.7" data-path="gzlm.html"><a href="gzlm.html#post-hoc-tests-1"><i class="fa fa-check"></i><b>3.5.7</b> Post Hoc Tests</a></li>
<li class="chapter" data-level="3.5.8" data-path="gzlm.html"><a href="gzlm.html#marginal-effects"><i class="fa fa-check"></i><b>3.5.8</b> Marginal Effects</a></li>
<li class="chapter" data-level="3.5.9" data-path="gzlm.html"><a href="gzlm.html#simple-effects-1"><i class="fa fa-check"></i><b>3.5.9</b> Simple Effects</a></li>
<li class="chapter" data-level="3.5.10" data-path="gzlm.html"><a href="gzlm.html#other-options-1"><i class="fa fa-check"></i><b>3.5.10</b> Other options</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gzlm.html"><a href="gzlm.html#ordinal"><i class="fa fa-check"></i><b>3.6</b> The Ordinal Model</a></li>
<li class="chapter" data-level="3.7" data-path="gzlm.html"><a href="gzlm.html#rationale-1"><i class="fa fa-check"></i><b>3.7</b> Rationale</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixa.html"><a href="appendixa.html"><i class="fa fa-check"></i><b>A</b> The <span class="math inline">\(R^2\)</span>’s</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendixa.html"><a href="appendixa.html#commuting-r2"><i class="fa fa-check"></i><b>A.1</b> Commuting <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="A.2" data-path="appendixa.html"><a href="appendixa.html#variance-explained"><i class="fa fa-check"></i><b>A.2</b> Variance explained</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="a1dummies.html"><a href="a1dummies.html"><i class="fa fa-check"></i><b>B</b> How many contrasts?</a>
<ul>
<li class="chapter" data-level="B.1" data-path="a1dummies.html"><a href="a1dummies.html#sufficiency-of-k-1-dummies"><i class="fa fa-check"></i><b>B.1</b> Sufficiency of K-1 dummies</a></li>
<li class="chapter" data-level="B.2" data-path="a1dummies.html"><a href="a1dummies.html#zero-intercept-anova"><i class="fa fa-check"></i><b>B.2</b> Zero-intercept ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixb.html"><a href="appendixb.html"><i class="fa fa-check"></i><b>C</b> Appendix Variable Types</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">GAMLj Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glm" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> The General Linear Model<a href="glm.html#glm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="draft"> Draft version, mistakes may be around </span></p>
<p><span class="keywords"> <span class="keytitle"> keywords </span> General Linear Model, Regression, ANOVA, Interaction, Moderation </span></p>
<div id="introduction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction<a href="glm.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The general linear model (GLM) encompasses the majority of the analyses that are commonly part of any practitioner’s statistical toolbox. There are good reasons for that: with a good knowledge of the GLM one can go a long way. Within the GLM one finds analyses such as simple and multiple regression, Pearson correlation, the independent-samples t-test, the ANOVA, the ANCOVA, and many of their derivations, such as mediation analysis, planned comparisons, etc (cf. <a href="booklet.html#naming">1.4</a>). The common theme of all these applications is that the dependent variable is continuous, hopefully normally distributed, and that the sample is composed of non-related, independent cases. The most basic yet very important application of the GLM is the simple regression, a GLM with one continuous independent variable (IV).</p>
</div>
<div id="regression" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> One continuous IV<a href="glm.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="oldnames"> <span class="oldnamestitle"> AKA </span> Simple Regression </span></p>
<p>Consider the dataset <code>manymodels</code> (cf. <a href="booklet.html#data">1.3</a>). The dependent variable is a continuous variable named <code>ycont</code>, and we want to estimate its linear relation with a continuous variable named <code>x</code>. The extensive relation between the two variables can be appreciated in a scatterplot. It is clear that <code>ycont</code> and <code>x</code> can be any variable, as long as we can consider them as continuous. For the sake of the argument, let us imagine that we went to a bar and measured <code>ycont</code> as the average number of smiles smiled by each customer in a given time and <span class="math inline">\(x\)</span> as the number of beers drunk for the same period.</p>
<p><img src="bookletpics/2_scatterplot1.png" width="596" /></p>
<p>What we want to know is the average increase (or decrease) of the dependent variable as the independent variable increases. Thus, how many smiles on average one should expect for one more beer? We ran a GLM to get the answer.</p>
<div id="glminput" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Input<a href="glm.html#glminput" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_menu1.png" width="285" /></p>
<p>We set the <code>ycont</code> variable as the dependent variable and the <code>x</code> variable as the independent continuous variable (see <a href="booklet.html#covnames">1.4.4</a>), and look at the results.</p>
<p><img src="bookletpics/2_input1.png" width="573" /></p>
</div>
<div id="model-recap" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Model Recap<a href="glm.html#model-recap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, we check out the <span class="tablename">Model Info</span> Table.</p>
<p><img src="bookletpics/2_output1.png" width="402" /></p>
<p>This is a recap table that says that we did what we wanted to do, and how we did it. The second table we get is the <span class="tablename">Model Fit</span> Table, where the <span class="math inline">\(R^2\)</span>, the adjusted <span class="math inline">\(R^2\)</span>, and their inferential test are presented.</p>
</div>
<div id="twofit" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Model Fit<a href="glm.html#twofit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_output2.png" width="456" /></p>
<p>The <span class="math inline">\(R^2\)</span> gives us the first glance of the model from the <em>variance angle</em> (cf. <a href="booklet.html#angles">1.4.1</a>). The short story says that our model (in this case the independent variable <span class="math inline">\(x\)</span>) explains, or accounts for, .323*100 percent of the variance. So, if all differences in the smiles (<code>ycont</code>) are set to 100, 32% of them can be associated with the number of beers drunk (<span class="math inline">\(x\)</span>). The <code>Adj.</code><span class="math inline">\(R^2\)</span> is the estimation of the variance explained by the model in the population, and the <code>df</code> is the number of parameters estimated by the model apart from the intercept: Here is one because we have one independent variable that requires only one coefficient. The <code>F</code> column gives the F-test testing the null hypothesis that <span class="math inline">\(R^2\)</span> is zero, and <code>p</code> is the probability of obtaining the observed <span class="math inline">\(R^2\)</span> under the null hypothesis. If you find this story a bit dull, you might want to read the full story in Appendix <a href="appendixa.html#appendixa">A</a>.</p>
</div>
<div id="omnibus-tests" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Omnibus Tests<a href="glm.html#omnibus-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_output3.png" width="507" /></p>
<p>With only one continuous variable this table is not very useful, but we comment on it anyway to get us familiar with the ideas of the two points of view always available in a linear model (cf <a href="booklet.html#angles">1.4.1</a>). The first line is there only for legacy compatibility purposes. It reports the inferential test of the model as a whole, which we already saw in <a href="glm.html#twofit">2.2.3</a>. The second line tells us the amount of variance of the dependent variable explained by the independent variable. In this case, the <span class="math inline">\(p\eta^2\)</span> is equal to the <span class="math inline">\(R^2\)</span>, because there is nothing to partial out (there is only one independent variable). Instructive, however, is to select the option <span class="math inline">\(\epsilon^2\)</span>.</p>
<p><img src="bookletpics/2_output4.png" width="585" /></p>
<p>we can notice that the <span class="math inline">\(\epsilon^2\)</span> is equal to the adjusted <span class="math inline">\(R^2\)</span>. Yes, that is going to stay: <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\eta^2\)</span> indices (partial or not) are the sample estimates of variance explained, whereas <span class="math inline">\(R_{adj}^2\)</span> and <span class="math inline">\(\epsilon^2\)</span> effect size indices (partial or not) are the population version (<span class="math inline">\(\omega^2\)</span> is population too). People tend to use the sample version of these indices (<span class="math inline">\(\eta^2\)</span> and <span class="math inline">\(R^2\)</span>) when they should use the population version (<span class="math inline">\(R^2_{adj}\)</span> and <span class="math inline">\(\epsilon^2\)</span>). The same goes for <span class="math inline">\(\omega^2\)</span> index, but you want to <a href='http://daniellakens.blogspot.com/2015/06/why-you-should-use-omega-squared.html' target='_blank'>read this about why you want to use them</a>, and <a href='https://gamlj.github.io/details_glm_effectsize.html' target='_blank'>this how they are computed</a> . In a nutshell, <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\eta^2\)</span> tell what happened in the sample, <span class="math inline">\(R_{adj}^2\)</span> and <span class="math inline">\(\epsilon^2\)</span> tell what should happen in the population.</p>
</div>
<div id="glmcoefs" class="section level3 hasAnchor" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Coefficients<a href="glm.html#glmcoefs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_output5.png" width="641" /></p>
<p>The regression coefficients table, here called the <span class="tablename">Parameters Estimates (Coefficients)</span> table, informs us about the size and the direction of the effect. The interesting coefficient is the one associated with the independent variable (the <code>Estimate</code> column). Here it is <span class="math inline">\(3.808\)</span>. This means that for every unit increase in the independent variable the dependent variable increases, on average, of <span class="math inline">\(3.808\)</span> units. In our toy example, for each beer one drinks, on average, one smiles <span class="math inline">\(3.808\)</span> smiles more. This is the <strong>regression coefficient</strong>, the very first and most solid pillar of the linear model. This interpretation is going to stick, so keep it in mind, because when models get more complex, we are going to amend it, but only to make it more precise, never to betray it.</p>
<p>The intercept, which is not focal here (<em>nobody looks at the intercept</em>), is worth mentioning for the sake of comparison with other software. If you run the same analysis in SPSS, R, Jasp, Stata, etc, you get the same <code>estimate</code> for the <code>x</code> variable, but a different <code>(Intercept)</code>. Recall that in any linear model the intercept is the expected value of the dependent variable for <span class="math inline">\(x=0\)</span>. In <span class="modulename">GAMLj</span>, however, the independent variables are centered to their means by default, so <span class="math inline">\(x=0\)</span> means <span class="math inline">\(x=\bar{x}\)</span>. So, in <span class="modulename">GAMLj</span> the intercept is the expected value of the dependent variable for the average value of <span class="math inline">\(x\)</span>.</p>
<p>Why centering? First, centering does not change the results of the regression coefficients of simple and multiple regression, so it is harmless in many situations. However, when an interaction is in the model, centering guarantees that the linear effects are the <em>main effects</em> one expects, and not some weird effects computed for the moderator equal to (possibly non-existing) zero. Furthermore, I believe that very few variables have a real and meaningful zero, so their mean is a more sensible value than zero <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. If your variables really have a meaningful zero (which you care about), you can always “unscale” your independent variables setting them to <code>Original</code> in the <span class="option">Covariates scaling</span> panel.</p>
<p><img src="bookletpics/2_input2.png" width="528" /></p>
</div>
<div id="pearson-correlation" class="section level3 hasAnchor" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Pearson Correlation<a href="glm.html#pearson-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Across sciences, the most used index of association between two variables is the Pearson Correlation, <span class="math inline">\(r\)</span>, otherwise named zero-order correlation, bivariate correlation, standardized covariance index, product-moment correlation, etc (pick any name, the Pearson correlation is a case of the <a href='https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy' target='_blank'>Stigler law of eponymy</a> anyway).</p>
<p>What is important here is that the Pearson correlation is just the standardized regression coefficient of a GLM with only two continuous variables (one <span class="tooltip">DV <span class="tooltiptext">Dependent Variable</span> </span>, on <span class="tooltip">IV <span class="tooltiptext">Independent Variable</span> </span>). In GLM terminology, it takes the name of <span class="math inline">\(\beta\)</span>. In our example, the correlation
between <code>ycont</code> and <code>x</code> is <span class="math inline">\(.568\)</span>. We can verify this by asking <span class="jamovi"><a href="http://www.jamovi.org" target="_blank">jamovi</a></span> to produce the correlation between the two variables in the <span class="option">Regression-&gt;Correlation Matrix</span> menu.</p>
<p><img src="bookletpics/2_output8.png" width="336" /></p>
<p>As expected, the correlation and the <span class="math inline">\(\beta\)</span> are the same. More specifically, the Pearson correlation is the regression coefficient that one obtains if the GLM is run after standardizing (computing the z-scores) both dependent and independent variable. This gives us a key to interpret the Pearson correlation in a precise way: Remembering that any standardized variable has 0 mean and standard deviation equal to 1, we can interpret the <span class="math inline">\(r\)</span> (and therefore the <span class="math inline">\(\beta\)</span>) as the number of standard deviations the dependent variable moves as we move the independent variable of one standard deviation. It varies from -1 to 1, with 0 meaning no relation.</p>
<p>When we deal with GLM with more than one independent variable, the link between the <span class="math inline">\(\beta\)</span> and the Pearson correlation is lost, but <span class="math inline">\(\beta\)</span>’s remain the coefficients obtained after standardizing the variables, so they remain the standardized coefficients.</p>
<p>If the user decides to report the <span class="math inline">\(\beta\)</span> coefficients, they would likely want to report the <span class="math inline">\(\beta\)</span> confidence intervals. They can be asked for in the input by flagging the “<span class="math inline">\(\beta\)</span> C.I. option”.</p>
<p><img src="bookletpics/2_simple_input9.png" width="535" />
<img src="bookletpics/2_simple_output9.png" width="808" /></p>
</div>
<div id="glmplot" class="section level3 hasAnchor" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Plots<a href="glm.html#glmplot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is always a good idea to look at your model results with a plot. A plot shows your fitted model (predicted values). Because a model is always an approximation of the real data, we want to show our predicted values against, or together, the actual data. In <span class="option">Plots</span> panel, we can set up a plot as follows:</p>
<p><img src="bookletpics/2_input3.png" width="564" /></p>
<p>and see the results:</p>
<p><img src="bookletpics/2_plot1.png" width="641" /></p>
<p>By default, the plot shows also the confidence bands around the regression line. The bands are the continuous version of the confidence intervals, and indicate the range of values where the predicted value are expected to lay.</p>
<p>Notice that the independent variable scale is centered to 0. This is because <span class="modulename">GAMLj</span> centers continuous variable by default (cf <a href="glm.html#glmcoefs">2.2.5</a>). If a plot with the original scale is preferred, one can flag the option <span class="option">X original scale</span> in the <span class="option">Plots</span> panel.</p>
</div>
</div>
<div id="multiregression" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> More continuous IVs<a href="glm.html#multiregression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="oldnames"> <span class="oldnamestitle"> AKA </span> Multiple Regression </span></p>
<p>Let us add to our model the <code>z</code> variable, again a continuous variable. To keep up with our toy example, let’s assume that <code>z</code> was a measure of participants’ extroversion.</p>
<p><img src="bookletpics/2_input4.png" width="582" /></p>
<p>The results are now updated. Let’s go through the most important tables.</p>
<div id="twofit2" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Model Fit<a href="glm.html#twofit2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_output6.png" width="454" /></p>
<p>The <span class="math inline">\(R^2\)</span> gives us the variance explained, or accounted for, of the dependent variable by the whole model. This means by both <code>x</code> and <code>z</code>, alone and together. The overall variance explained is statistically different from zero, so we can say we do explain some variance of <em>smiles</em> (<code>ycont</code>) thanks to the variability of <em>beers</em> (<code>x</code>) and <em>extroversion</em> (<code>z</code>). The question is now how each independent variable contributes to this variance explained. We need to look at the individual contributions, so the <span class="option">Omnibus Tests</span>.</p>
</div>
<div id="variances" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Omnibus Tests<a href="glm.html#variances" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_output7.png" width="614" /></p>
<p>Please notice that I selected <span class="math inline">\(\eta^2\)</span>, <code>partial</code> <span class="math inline">\(\eta^2\)</span>, <span class="math inline">\(\epsilon^2\)</span>, and <code>partial</code> <span class="math inline">\(\epsilon^2\)</span>, so we can interpret these indices. Before that, however, we can mention that the <code>x</code> variable effect is statistically different from zero, F(1,117)=58.87, p.&lt;.001, whereas the effect of <code>z</code> reaches the predefined level of significance by a very tiny margin, F(1,117)=4.242, p.=.042. So we can say that there is enough evidence to believe that both effects are different from zero, although the former seems more solid than the latter. Statistical significance, however, is only a part of the story: effects should be evaluated on at least three dimensions: <strong>significance</strong>, <strong>size</strong>
and <strong>direction</strong>. We now want to evaluate their size.</p>
<p>Effect size indexes are good tools to evaluate effect sizes (<em>nomen omen</em>). We start with the <code>partial</code> <span class="math inline">\(\eta^2\)</span>, mainly because it is the most used and reported one effect size index in the literature (I always thought that is the case because it is the only ES produced by SPSS GLM). The <code>partial</code> <span class="math inline">\(\eta^2\)</span> is the proportion of variance uniquely accounted for by the independent variable, expressed as the proportion of the variance of the dependent variable not explained by the other independent variables. In short, for <code>x</code> (<em>beers</em>) is the proportion of variance of <em>smiles</em> not explained by <em>extroversion</em> that is explained by <em>beers</em>. In other words, it answers the question: if everybody had the same level of <em>extroversion</em>, how much variance would <em>beers</em> explain?</p>
<p>Notice that the unique variance explained by <em>beers</em> (<code>x</code>), namely 31.2%, is computed after removing the variance explained by <em>extroversion</em> (<code>z</code>). Often you want to know how much variance a variable explains of the total variance, so of all the variance of the dependent variable. That is the <span class="math inline">\(\eta^2\)</span>, the variance uniquely explained by a variable as a proportion of the total variance of the dependent variable. In other words, it answers the question: how much variance of <em>smiles</em> would <em>beers</em> uniquely explain?</p>
<p>The <span class="math inline">\(\epsilon^2\)</span> and <code>partial</code> <span class="math inline">\(\epsilon^2\)</span> indexes can be interpreted as the <span class="math inline">\(\eta\)</span>’s, but they are adjusted to represent the population variances, not the sample variances. So, they are better estimations of the “real” effect size indexes.</p>
<p>A detailed description of the computation of these indexes can be found in <a href='https://gamlj.github.io/details_glm_effectsize.html' target='_blank'>GAMLj help page</a></p>
</div>
<div id="household-chores" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Household Chores<a href="glm.html#household-chores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Effect size indexes (partial vs not partial ones) are usually explained with Venn Diagrams. We try here without them. Assume you leave in a house with another person. There are 100 chores to do in your household, such us washing the dishes, cleaning the windows, and taking the dog out for a walk. You do 20 chores, 5 of which you did together with your companion. Your companion did 40 chores (they are always better), including the ones you did together. So, altogether you people did 55 chores, your companion did 35 alone, you did 15 alone, and 5 chores were done together.</p>
<p>As a couple, your overall contribution is 55/100, so your <span class="math inline">\(R^2=.55\)</span>. You alone did 15 chores, so your unique contribution is <span class="math inline">\(\eta^2=15/100=.15\)</span> of the total amount of chores to be done. However, of the chores left to do by your companion (60), you did alone 15, so your <em>partial</em> contribution is <span class="math inline">\(p\eta^2=15/60=.25\)</span>. So the difference between <span class="math inline">\(\eta^2\)</span> (or any non-partial ES) and its partial version is the denominator. Non-partial indexes are proportions of the total, partial ones are proportions of the total minus the part accounted for by the others.</p>
<p>In any household, we would use the <span class="math inline">\(\eta^2\)</span>, but many authors are still using the <span class="math inline">\(p\eta^2\)</span>. What is important is to know the difference between the two computation methods, so we can feel free to use which one we prefer. A deep discussion of this matter can be found in <span class="citation">Olejnik and Algina (<a href="#ref-olejnik2003generalized" role="doc-biblioref">2003</a>)</span>, which I recommend reading.</p>
</div>
<div id="coefficients" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Coefficients<a href="glm.html#coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now we look at the direction and intensity of the effects, interpreting the <span class="tablename">Parameters Estimates (Coefficients)</span> Table.</p>
<p><img src="bookletpics/2_output9.png" width="643" /></p>
<p>For each variable effect, we see the <code>Estimate</code> (usually called the <span class="math inline">\(b\)</span> coefficients), the standard error (<code>SE</code>), the confidence intervals, the <span class="math inline">\(\beta\)</span>, the degrees of freedom (<code>df</code>), the t-test (<code>t</code>) and the p-values (<code>p</code>). So, the full set of estimations (<span class="math inline">\(b\)</span> and <span class="math inline">\(\beta\)</span>) and the inferential tests (t-test, C.I. df and p-values). Let’s interpret them for <span class="math inline">\(x\)</span>:</p>
<ul>
<li><span class="math inline">\(b\)</span> : keeping constant <span class="math inline">\(z\)</span>, for one unit more in <span class="math inline">\(x\)</span> we expect the dependent variable average score to increase of <span class="math inline">\(3.777\)</span> units.</li>
<li><span class="math inline">\(\beta\)</span> : the effect corresponds to a standardized coefficient of <span class="math inline">\(.564\)</span>, indicating that for one standard deviation more in <span class="math inline">\(x\)</span>, <span class="math inline">\(ycont\)</span> increases of .564 standard deviations, keeping constant the effect of <code>z</code>.</li>
<li><span class="math inline">\(C.I.\)</span> : “Were this procedure to be repeated on numerous samples, the proportion of calculated 95% confidence intervals that encompassed the true value of the population parameter would tend toward 95%” (cf. <a href='https://en.wikipedia.org/wiki/Confidence_interval' target='_blank'>Wikipedia</a>). Weird? Yes, that’s what confidence intervals are. But what about 2.785 and 4.769? Well, they are the two values that include 95% of the cases of the distribution of sample estimates if, and only if, we got exactly the right population parameter in our sample. But we have no way to assess whether we did, so there is not much to learn from these two numbers. This book is not the right place to discuss this issue, but interested readers may enjoy reading <span class="citation">Mayo (<a href="#ref-mayoci" role="doc-biblioref">1981</a>)</span>.</li>
</ul>
</div>
</div>
<div id="moderation" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Continuous IVs and interaction<a href="glm.html#moderation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous model, we can make a plot of the effect of beers (<code>x</code>) on smiles (<code>ycont</code>), at different levels of extroversion (<code>z</code>). This can be achieved in <span class="modulename">GAMLj</span> by asking for a plot as follows:</p>
<p><img src="bookletpics/2_input10.png" width="562" /></p>
<p><img src="bookletpics/2_output10.png" width="642" /></p>
<p>The three lines depicted in the plot are the effects of <code>x</code> on <code>ycont</code>, estimated at different levels of <code>z</code>. Because <code>z</code> is continuous, <span class="modulename">GAMLj</span> automatically sets the focal levels of <code>z</code> equal to the mean, on SD above the mean, and one SD below the mean (this can be changed, see below).</p>
<p>As expected, the three effects depicted are perfectly parallel. This is because in multiple regression, the effect of each variable is computed keeping constant the other variable, which is equivalent to saying that the effects are computed as if they were the same at each level of the other variable. In other words, they are forced to be the same, no matter the data. But this constrain can be removed by adding an interaction in the model. An interaction allows the effect one variable to change at different levels of the other. In other words, an interaction allows the effect of <code>x</code> to depend on the levels of <code>z</code>. The interaction coefficient tells us how much the effect of one variable changes at different levels of the other. Let’s do it.</p>
<p>In the <span class="option">Model</span> panel, we select both variables on the left field and move them together to the right field, defining an interaction <span class="math inline">\(x*z\)</span>.</p>
<p><img src="bookletpics/2_input11.png" width="554" /></p>
<p>The results updated accordingly showing now also the interaction effects.</p>
<p><img src="bookletpics/2_output11.png" width="662" /></p>
<p>Let’s focus on the <span class="tablename">Parameter Estimates (Coefficients)</span> Table. When there is an interaction in a linear model, the effect associated with the independent variables are the effect of the independent variable computed for the other variable equal to zero. This is not a software choice, that is the way a linear model is <span class="citation">(<a href="#ref-aiken1991multiple" role="doc-biblioref">Aiken, West, and Reno 1991</a>)</span>. However, <span class="modulename">GAMLj</span>, by default, centers the continuous variables to their means, so the linear effects can be interpreted as <em>main effects</em>, namely, the effect of the variable computed on average, at the average level of the other variable. Thus, we can say that, <em>on average</em>, beers (<span class="math inline">\(x\)</span>) has a positive effect on smiles (<span class="math inline">\(ycont\)</span>) of <span class="math inline">\(3.686\)</span>, corresponding to a standardized effect of <span class="math inline">\(.550\)</span>. For the average level of beers (<span class="math inline">\(x\)</span>), the effect of extroversion (<span class="math inline">\(z\)</span>) is <span class="math inline">\(.954\)</span>, corresponding to a standardized effect of <span class="math inline">\(.156\)</span>.</p>
<p>As regards the interaction effect, <span class="math inline">\(x*z\)</span>, it tells us that the effect of beers (<span class="math inline">\(x\)</span>) increases of <span class="math inline">\(1.028\)</span> units for each unit more of extroversion (<span class="math inline">\(z\)</span>). We see that this effect is statistically significant, so we can say that the effect of beers changes at different levels of extroversion. We should make clear that the same interpretation is valid if we invert <span class="math inline">\(z\)</span> with <span class="math inline">\(x\)</span>. The effect of <span class="math inline">\(z\)</span> is different at different levels of <span class="math inline">\(x\)</span>.</p>
</div>
<div id="modint" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Moderation=interaction<a href="glm.html#modint" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many authors calls this type of effect a moderation effect. They are correct, but there is nothing special about interactions between continuous variables. Any interaction effect, no matter if the independent variables are continuous or categorical, tests a moderation model. The only difference between moderation and interaction is theoretical. When we lay out a theoretical model, we declare which variable is the hypothesized moderator and which is the independent variable. Statistically, they are equivalent, and the strength of moderation is tested with an interaction. We will see that moderation models are tested also with categorical variables (within what people calls <em>ANOVA</em>) or when one variable is categorical and the other is continuous (within what people used to call <em>ANCOVA</em>, but not anymore). We have seen that naming techniques rather than models is cumbersome (<a href="booklet.html#naming">1.4</a>). The same goes for <em>moderation</em>. Just keep in mind that moderation refers to a theoretical model, its statistical counterpart is <em>interaction</em>, for all kinds of variables.</p>
</div>
<div id="simpleslopes" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Simple Slopes<a href="glm.html#simpleslopes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When an interaction is present (let’s say it is significantly different from zero), we can probe it <span class="citation">(<a href="#ref-aiken1991multiple" role="doc-biblioref">Aiken, West, and Reno 1991</a>)</span>. Probing means to estimate, test, and visualize the effect of one independent variable at different levels of the other. The “other variable” is usually called the <em>moderator</em>: The variable that is supposed to change the effect of the independent variable. In our example, we focus on the effect of beers (<span class="math inline">\(x\)</span>), and different levels of the moderator extroversion (<span class="math inline">\(z\)</span>). First, let’s look at the plot.</p>
<p><img src="bookletpics/2_input10.png" width="562" /></p>
<p><img src="bookletpics/2_output12.png" width="628" /></p>
<p>We can see now that the lines representing the effect of <span class="math inline">\(x\)</span> at different levels of <span class="math inline">\(z\)</span> are no longer parallel, they have <em>different slopes</em>. Looking at the plot, we can see that the effect of <span class="math inline">\(x\)</span> is stronger for high levels of <span class="math inline">\(z\)</span> (<code>Mean+1*SD</code>), than for the average level of <span class="math inline">\(z\)</span> (<code>Mean</code>) than for low levels of <span class="math inline">\(z\)</span> (<code>Mean-1*SD</code>).</p>
<p>The plot is very useful to visualize how the effect of one variable changes at different levels of the moderator. Often, however, we also want to estimate those slopes and maybe test them. We can do that in the <span class="option">Simple Effects</span> panel. Recall that <em>simple slopes</em> is just a name for <em>simple effects</em> applied to continuous variables, so <span class="modulename">GAMLj</span> uses the term <em>simple effects</em> because it generalizes to any type of independent variable.</p>
<p><img src="bookletpics/2_input13.png" width="563" /></p>
<p><img src="bookletpics/2_output13.png" width="737" /></p>
<p>The first table reports the F-tests and indices of explained variance (<a href="glm.html#variances">2.3.2</a>). The coefficients table reports the slopes of the effect of the <code>x</code> variable at different levels of the <code>z</code> variable. Practically, the tables report the effect sizes and the inferential tests associated with the lines depicted in the plot.</p>
<p>If one wants to change the levels of the moderator at which the effects are estimated and plotted, one can go to the <span class="option">Covariate Scaling</span> panel and change the <code>Covariate conditioning</code> setup. For instance, one may want to explore the effect of <code>x</code> at 2 SD away from the mean, so one sets the field under Mean <span class="math inline">\(\pm\)</span> SD to 2. One can also decide to condition the slopes to specific percentiles of the <code>z</code> variable, by selecting Percentiles <span class="math inline">\(\pm\)</span> SD, which conditions to 25th, 50th, 75th percentile (cf <a href='https://gamlj.github.io/glm.html' target='_blank'>GAMLj help page</a>).</p>
<p><img src="bookletpics/2_input14.png" width="559" /></p>
<p><img src="bookletpics/2_output14.png" width="736" /></p>
<p>We can be happy with the analysis, as we have estimated, tested, and quantified all the interesting effects of our independent variables on the dependent variables. We discuss simple effects again in <a href="glm.html#simpleinteractions">2.13</a>. An equivalent example, with different data, can be found in <a href='https://gamlj.github.io/glm_example1.html' target='_blank'>GAMLj help page: GLM example 1</a>.</p>
</div>
<div id="anova" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Categorical IVs<a href="glm.html#anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="oldnames"> <span class="oldnamestitle"> AKA </span> ANOVA </span></p>
<p>We can now work with a model with categorical independent variables. The dataset provides <code>cat2</code> and <code>cat3</code> variables, with two groups and three groups respectively. Their combination produces the following groups.</p>
<p><img src="bookletpics/2_anova_output1.png" width="407" /></p>
<p>To put some flesh on the bones, let’s imagine that cat3 be the type of beer one drinks, with levels <em>stout</em>, <em>IPA</em> and <em>pilsner</em>. Assume <code>cat2</code> be type of bar, <em>music bar</em> vs <em>sports bar</em>. To remember, let’s put some labels on the variables levels.</p>
<p><img src="bookletpics/2_anova_input1.png" width="773" />
<img src="bookletpics/2_anova_input2.png" width="753" /></p>
<p>Take a note about the fact that the specific values present in the dataset of a categorical variable have no bearing on the results of the analysis. The categorical variables are coded by <span class="modulename">GAMLj</span> independently of their values: It applies a coding system to cast the categorical <span class="tooltip">IV <span class="tooltiptext">Independent Variable</span> </span> into the model. We can also change the coding system with the module options (see below).</p>
<p>We can now run a new model, with <code>ycont</code> as dependent variable, and the two categorical variables as the independent ones.</p>
<p><img src="bookletpics/2_anova_input3.png" width="572" /></p>
<p>The tables we have seen for the first model are the same produced now, we only need to adjust some interpretation to reflect the categorical nature of the variables.</p>
<div id="model-fit-and-omnibus-tests" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Model Fit and Omnibus tests<a href="glm.html#model-fit-and-omnibus-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_anova_output2.png" width="464" /></p>
<p><span class="tablename">Model Fit</span> and <span class="tablename">ANOVA Omnibus tests</span> tables do not require adjustments of the interpretation. Here we see that our model explains <span class="math inline">\(.162*100\)</span> percent (<span class="math inline">\(R^2\)</span>) of the dependent variable variance, <span class="math inline">\(.125*100\)</span> percent as population estimate (Adj. <span class="math inline">\(R^2\)</span>), and that the type of bar (<code>cat2</code>) has a main effect, type of beer (<code>cat3</code>) has a main effect, and there is no indication of an interaction. Effects are small, with <code>cat2</code> main effect explaing <span class="math inline">\(.112*100\)</span> percent of the variance not explained by the other effects, <code>cat3</code> main effect explaining <span class="math inline">\(.056*100\)</span> percent, and the interaction explaining only around 2% of the partial variance.</p>
</div>
<div id="dummies" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Coefficients<a href="glm.html#dummies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When dealing with categorical independent variables, one usually does not look at the coefficients, but one goes straight to the plots to interpret the results. Nonetheless, the coefficients are present and they can be interpreted.</p>
<p><img src="bookletpics/2_anova_output3.png" width="891" /></p>
<p>Their interpretation depends on the way the levels (groups) of the variables are coded. In fact, to cast a categorical variable into a linear model (any linear model), it must be coded with weights (numbers) that represent specific contrasts. We need <span class="math inline">\(K-1\)</span> contrasts to represent <span class="math inline">\(K\)</span> groups (see appendix <a href="a1dummies.html#a1dummies">B</a> for more details). These contrasts are commonly called <em>dummy variables</em>, but it is more correct to call them <em>contrast variables</em>. <span class="modulename">GAMLj</span> default uses the <em>simple</em> coding system, as it is evident in the <span class="option">Factor Coding</span> tab.</p>
<p><img src="bookletpics/2_anova_input4.png" width="560" /></p>
<p><em>Simple</em> coding contrasts estimate the mean difference between one reference group and each of the other groups. The first level present in the dataset is used as reference group. <em>Simple</em> coding yields the same comparisons as the more classical <em>dummy</em> system, but <em>simple</em> coding centers the contrasts to zero, so in the presence of an interaction in the model, the main effects are correctly estimated as <em>average effects</em>. So, for <code>cat2</code>, we need only one <em>contrast</em> which compares <em>music</em> vs <em>sports</em>. The coefficient <span class="math inline">\(4.343\)</span> is the mean difference (in the dep variable) between the two groups. So, people in the <em>music</em> bar smile 4.343 smiles more than people in the <em>sports</em> bar. For type of beer (<code>cat3</code>), we need two contrast variables, because we have three groups: <code>cat31</code> compares the reference group <em>pilsner</em> against <em>IPA</em>, the second contrast <code>cat32</code> compares <em>pilsner</em> with <em>stout</em>. The remaining coefficients are required to estimate the interaction <code>cat2 X cat3</code>.</p>
<p>We should notice that the model does not estimate all possible contrasts, for instance <em>stout</em> is not compared with <em>IPA</em>. The reason is that those contrasts are redundant in order to capture the whole explained variance (cf. Appendix <a href="a1dummies.html#a1dummies">B</a>). If one needs all possible comparisons, one can use <span class="option">Post Hoc Tests</span> panel to obtain the comparisons (cf. <a href="glm.html#posthoc">2.8</a>).</p>
<p><span class="modulename">GAMLj</span> offers several coding systems to code the categorical variables. If you want to take a look at what the contrasts are comparing, you can ask for <span class="option">Contrast Coefficients tables</span>, so a table visualizing the actual coding is produced.</p>
<p><img src="bookletpics/2_anova_output4.png" width="417" /></p>
<p>All coding system used in <span class="modulename">GAMLj</span> are explained in details in the <a href='https://gamlj.github.io/rosetta_contrasts.html' target='_blank'>GAMLj help page: contrasts</a>.</p>
</div>
<div id="plots" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Plots<a href="glm.html#plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As for the continuous <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span> case, we can plot the results. When the <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span> are categorical, we obtain the plot of the means.</p>
<p><img src="bookletpics/2_anova_input5.png" width="551" />
<img src="bookletpics/2_anova_output5.png" width="674" /></p>
<p>From the results is evident the main effect of <code>cat2</code>, with <em>music</em> bar showing a higher average than <em>sports</em> bar, and a small main effect of type of beer, with <em>stout</em> vaguely larger than <em>IPA</em> and larger than <em>pilsner</em>.</p>
</div>
</div>
<div id="posthoc" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Post-hoc tests<a href="glm.html#posthoc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes people want to probe main effects or interactions of categorical variables to test all possible comparisons among means. It should not be a habit to do so, because the coefficients table already provides comparisons that may be enough to explain the results. One can also use a simple effects analysis to test specific comparisons. Nonetheless, if one really needs all possible comparisons, one can use the <span class="option">Post Hoc Tests</span> panel. Here we ask for the <em>post hoc</em> tests of the variable <code>cat3</code>, because <code>cat2</code> features only two levels, so probing is useless (we have already its main effect). In this example we do not probe the interaction, because it is very small and not significant, but the module allows probing all possible effects.</p>
<p><img src="bookletpics/2_anova_input6.png" width="570" />
<img src="bookletpics/2_anova_output6.png" width="787" /></p>
<p>Post hoc tests are basically t-tests comparing any pair of levels of the independent variables in their dependent variable means. So, here we have the mean for the <em>pilsner</em> group compared with the <em>IPA</em> group, the <em>pilsner</em> group compared with the <em>stout</em> group, and the <em>IPA</em> group compared with the <em>stout</em> group. For each comparison we have the difference in means, the confidence intervals, the t-test, df, and p-value. All columns report what a simple t-test would yield, but the p-value column is different. The p-value is adjusted for multiple comparisons, meaning that the p-value is calculated to counteract the higher probability of finding something singificant when multiple tests are run. The adjustment is proportional to the number of comparisons that are tested.</p>
<p>Why adjusting? Well, adjustment is required when the researcher does not have an <em>a priori</em> hypothesis regarding which comparison should be <em>significant</em> and which should not. If one does not have a clear hypothesis, any comparison that comes out as significant will be considered as <em>real</em>, so different from zero. However, when several comparisons are tested, the probability of obtaining at least one comparison as significant is not .05 (<span class="math inline">\(\alpha\)</span>), as one expects, but higher: the more comparisons one tests, the higher the probability.</p>
<p>Recall that any inferential test (<em>frequentist</em> tests, I should add) lays out a null-hypothesis, say <span class="math inline">\(\Delta=0\)</span> (difference equal to zero). The t-test returns the probability of obtaining the observed result (here <span class="math inline">\(-2.788\)</span> for <em>pilsner</em> vs <em>stout</em>) if we were sampling from a distribution in which <span class="math inline">\(\Delta=0\)</span>. When the p-value is low, we say that it is very unlikely that our result comes from a distribution where <span class="math inline">\(\Delta=0\)</span>, so we reject the null-hypothesis. Unlikely, however, does not mean impossible, so there is always a chance to be wrong in rejecting the null-hypothesis. If we use a significance cut-off of <span class="math inline">\(\alpha=.05\)</span>, we accept the risk of being wrong 5% of the time, if the population difference is indeed zero. The good news is that we’ll be right <span class="math inline">\(1-\alpha=.95\)</span> (*100) of the times. However, this reasoning is valid for one test. If we run two tests, we want to take the right decision for boths, so the probability of being right in both tests is <span class="math inline">\((1-\alpha)^2=0.902\)</span>. If we run three tests, we will be right with probability <span class="math inline">\((1-\alpha)^3=0.857\)</span>. So, we capitalize on chance, meaning that it becomes more and more likely to get at least one test as significant, even if they all come from a population where no difference is present.</p>
<p>Multiple comparisons adjustment corrects the p-value in order to make a significant result more difficult to obtain. Practically, the p-value is increased proportionally to the number of comparisons that are tested. There are different methods to adjust the p-value, and they are listed in the panel as options: <em>Bonferroni</em>, <em>Tukey</em>, <em>Holm</em>, <em>Scheffe</em>, <em>Sidak</em>. They are all alternative ways to adjust the p-value. The interesting dimension along which they differ is liberalism-conservativism. In statistics, a <em>liberal</em> test is more likely to yield a significant result than a conservative test, <em>ceteris paribus</em>. Liberal tests are more powerful but their correction of the p-value may not be enough, whereas conservative tests correct for multiplicity but may result under-powered. Bonferroni and Sidak adjustment tend to be more conservative than the others. <em>Tukey</em> correction seems the more reasonable choice in most circumstances <span class="citation">(<a href="#ref-midway2020comparing" role="doc-biblioref">Midway et al. 2020</a>)</span>.</p>
<p>Recall, post hoc tests are needed when the researcher is willing to accept any significant result as worth mentioning and interpreting. On the other hand, if one has a clear pattern of means hypothesized, adjustment may not be needed and the specific comparisons may be evaluated without correction.</p>
</div>
<div id="cohens-d" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Cohen’s d<a href="glm.html#cohens-d" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cohen’s <span class="math inline">\(d\)</span> is probably the most used effect size index to quantify a mean difference. It expresses the mean difference in a standardized scale: It is the ratio of the mean difference over the within groups standard deviation, or residual standard deviation. Unfortunately, Cohen <span class="citation">(<a href="#ref-cohen2013statistical" role="doc-biblioref">J. Cohen 2013</a>)</span> defined the <span class="math inline">\(d\)</span> index for the population, and thus it is not clear how to compute it in the sample. <span class="modulename">GAMLj</span> offers three options.</p>
<ul>
<li><p>Cohen’s d (model SD) <span class="math inline">\(d_{mod}\)</span>: the means difference is divided by the estimated standard deviation computed based on the model residual variance.</p></li>
<li><p>Cohen’s d (sample SD) <span class="math inline">\(d_{sample}\)</span>: the means difference is divided by the pooled standard deviation computed within each group.</p></li>
<li><p>Hedge’s g <span class="math inline">\(g_{sample}\)</span>: the means difference is divided by the pooled standard deviation computed within each group, corrected for sample bias. The correction is the one describe by <span class="citation">Hedges and Olkin (<a href="#ref-hedges2014statistical" role="doc-biblioref">2014</a>)</span> based on the Gamma function.</p></li>
</ul>
<p><img src="bookletpics/2_anova_output8.png" width="1031" /></p>
<p>The two d’s differ in their adherence with the model being estimated. The <em>model SD</em> version, gives the estimated <code>d</code> after removing the variance explained by the other effects in the model, so it is the actual effect size of the comparison of the estimated marginal means. The <em>sample SD</em> gives the crude standardized difference, as if the model was not estimated at all, but only the two groups were considered. The <em>model SD</em> should be preferred as default index to report in a model, the <em>sample SD</em> version can be useful to compare effects in the literature obtained with a different model or without a model.</p>
<p>Hedge’s g gives a population estimate of the sample <span class="math inline">\(d\)</span>.</p>
</div>
<div id="estimated-marginal-means" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Estimated marginal means<a href="glm.html#estimated-marginal-means" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The means that are plotted in the plot can be visualized, with their standard error and confidence intervals, by defining the variables for which we want them in the <span class="option">Estimated Marginal Means</span>.</p>
<p><img src="bookletpics/2_anova_input7.png" width="570" />
<img src="bookletpics/2_anova_output7.png" width="524" /></p>
<p>In balanced designs with only categorical <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span>, they are the means of the groups (or combinations of groups). When there are also continuous <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span>, they are adjusted for the continuous variables: They are the means estimated after keeping constant the continuous independent variables.</p>
<p>If marginal means are requested for a continuous variable, they represent the expected value of the dependent variable for three <em>interesting</em> levels of the independent variable, where the <em>interesting values</em> are defined as for the <em>simple slope</em> technique (cf <a href="glm.html#simpleslopes">2.6</a>)</p>
<p>For instance, in model <a href="glm.html#moderation">2.4</a>, if we ask for the estimated marginal means for <span class="math inline">\(x\)</span>, we obtain the following estimates:</p>
<p><img src="bookletpics/2_moderation_output1.png" width="514" /></p>
<p>meaning that, based on the model, we expect the number of smiles (<code>ycont</code>) to be 28.03 for low level of beers (1 SD below average of <code>x</code>), 31.7 for the average level of beers (average of <code>x</code>), and 35.9 for high levels of beers (1 SD above average of <code>x</code>).</p>
</div>
<div id="ancova" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Categorical and Continuous IVs<a href="glm.html#ancova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="oldnames"> <span class="oldnamestitle"> AKA </span> ANCOVA </span></p>
<p>We now insert in the model also <span class="math inline">\(x\)</span>, so we have both categorical and continuous <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span>. This model was once called <em>ANCOVA</em>, but it did not allow for interactions. We simply call it a GLM.</p>
<p><img src="bookletpics/2_ancova_input1.png" width="586" /></p>
<p>Keeping up with an old (SPSS?) tradition, <span class="modulename">GAMLj</span> does not automatically insert into the model interactions involving a continuous variable, so if one needs them, they should be added manually (see below). For the moment, here is our model.</p>
<p><img src="bookletpics/2_ancova_input2.png" width="577" /></p>
<div id="results-tables" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> Results tables<a href="glm.html#results-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="bookletpics/2_ancova_output1.png" width="794" /></p>
<p>Combining categorical and continuous <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span> does not really change the way we interpret the results. We interpret the continuous variable effect like we did for the continuous variables GLM (<a href="glm.html#multiregression">2.3</a>) and the categorical independent variables effects as we did for the GLM with categorical variables (<a href="glm.html#anova">2.7</a>). So, in the <span class="tablename">Model Fit</span> Table we found the variance explained by all the effects combined, in the <span class="tablename">ANOVA Omnibus Tests</span> Table we find the explained variances and their tests, and in the <span class="tablename">Parameter Estimates (Coefficients)</span> Table we find the coefficients. We just need to keep in mind that all the effects are computed keeping constant the other variables, so we can use this kind of model to <em>covariate</em> variables that may have spurious effects. That is why in the last century this model was called <em>ANalysis of COVAriance</em>. At that time, one assumption of this analysis was that there was no interaction between the categorical variables and the continuous variables. Nowadays, we can release the assumption, and just test the interaction, in case is there.</p>
</div>
</div>
<div id="moderation2" class="section level2 hasAnchor" number="2.12">
<h2><span class="header-section-number">2.12</span> Categorical and Continuous Interactions<a href="glm.html#moderation2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is nothing special about interactions between continuous and categorical variables, they just test a moderation model. To obtain the interactions we select all variables in the <span class="option">Model</span> panel and click the arrow to bring them in the <code>Model Terms</code> field.</p>
<p><img src="bookletpics/2_ancova_input3.png" width="567" /></p>
<p>Upon updating the model, the results update as well, and now we can check the main effects, the 2-way interactions and the 3-way interaction. We focus on the variances explained and F-tests.</p>
<p><img src="bookletpics/2_ancova_output3.png" width="499" /></p>
<p>When the model features interactions of different orders, it is a good idea to start the interpretation from the highest order interaction, in our case the 3-way interaction. Here it seems to be different from zero, F(2,108)=6.340, p=.002, <span class="math inline">\(p\eta^2=.105\)</span>.
This means that the 2-way interaction <code>x * cat3</code> is different at different levels of <code>cat2</code>. In general, a 3-way interaction can be interpreted by picking a moderator (any will do, the interaction is symmetrical), and saying that the other two variables interaction changes at different levels of the moderator.</p>
<p>Upon finding a higher order interaction, one wants to plot it and interpret its direction. This is because the higher order interaction shows a pattern of results that is more specific then lower order effects. In fact, lower order effects are always interpreted <em>on average</em>, so they are less specific than the higher order effect. Practically, the <code>cat2 * cat3</code> significant interaction in our results is not very informative given these results, because it says that <em>on average</em>, meaning averaging across levels of <code>x</code>, there is an interaction between type of bar and type of beer. But we know from the significant 3-way interaction that the 2-way interaction changes at different levels of <code>x</code>, so it is not really important its value on average.</p>
<p>If the 3-way interaction was not significant, or minuscule for our standards, we would simply start probing the lower order effects.</p>
<div id="plot" class="section level3 hasAnchor" number="2.12.1">
<h3><span class="header-section-number">2.12.1</span> Plot<a href="glm.html#plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To visualize the 3-way interaction, we should pick the two variables that create the 2-way interaction we want to explore, and a moderator: the 2-way interaction is displayed for different levels of the moderator. Here we want to see the effect of beers (<code>x</code>) by type of beers (<code>cat3</code>), displayed at different levels of type of bar <code>cat2</code>.</p>
<p><img src="bookletpics/2_ancova_input4.png" width="564" />
<img src="bookletpics/2_ancova_output4a.png" width="623" />
<img src="bookletpics/2_ancova_output4b.png" width="653" /></p>
<p>The interpretation can follow these lines: In <em>sports bar</em>, the effect of beers on smiles is generally positive, it is stronger for <em>stout</em> and IPA beers, and weaker for <em>pilsner</em>. In <em>music bar</em>, the effect of beers on smiles is positive as well, but stronger for <em>stout</em> and weaker and very similar for <em>pilsner</em> and <em>IPA</em>. The fact that the two patterns can be described differently across types of bar is justified by the significant 3-way interaction.</p>
</div>
<div id="simple-effects" class="section level3 hasAnchor" number="2.12.2">
<h3><span class="header-section-number">2.12.2</span> Simple Effects<a href="glm.html#simple-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can quantify and tests all the effects depicted in the plots by asking for <em>simple effects</em> analysis. We just need to pick the variable for which we want to study the effects, and select the moderators: the focal variable effect will be estimated and tested for each combination of the moderators values.</p>
<p><img src="bookletpics/2_ancova_input5.png" width="562" />
<img src="bookletpics/2_ancova_output5.png" width="818" /></p>
<p>The interpretation of these effects follows what we have done for the 2-way interaction before (<a href="glm.html#simpleslopes">2.6</a>). However, because we have a 3-way interaction, we can also probe for <em>simple interactions</em>.</p>
</div>
</div>
<div id="simpleinteractions" class="section level2 hasAnchor" number="2.13">
<h2><span class="header-section-number">2.13</span> Simple Interactions<a href="glm.html#simpleinteractions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Simple interactions</em> are simple effects, in which the focal effect is an interaction. We can ask for this analysis by selecting <span class="option">Simple Interaction</span> option.</p>
<p><img src="bookletpics/2_ancova_input6.png" width="567" />
<img src="bookletpics/2_ancova_output6.png" width="803" /></p>
<p>This analysis is useful to probe high order interaction. Here we see the <code>x * cat3</code> interaction computed at the two levels of <code>cat2</code>. So, we can say that for the group <em>music</em>, the interaction is present, whereas for group <em>sports</em>, the <code>x * cat3</code> interaction is not statistically significant.</p>
</div>
<div id="model-comparison-approach" class="section level2 hasAnchor" number="2.14">
<h2><span class="header-section-number">2.14</span> Model-comparison approach<a href="glm.html#model-comparison-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-method" class="section level3 hasAnchor" number="2.14.1">
<h3><span class="header-section-number">2.14.1</span> The Method<a href="glm.html#the-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have touched upon the fact that many tests and indices in the linear model can be derived by the comparison between two models, one being our full model, the other being a nested model <span class="citation">(<a href="#ref-judd2017data" role="doc-biblioref">Judd, McClelland, and Ryan 2017</a>)</span>, (cf. Appendix <a href="appendixa.html#appendixa">A</a>). A nested model is simply a model that, compared with the full model, lacks some terms, and does not have any term not present in the full model. <span class="modulename">GAMLj</span> allows custom model comparisons by flagging the option <span class="option">Activate</span> under <span class="option">Model Comparison</span>.</p>
<p><img src="bookletpics/2_modelcomparison_input1.png" width="563" /></p>
<p>Comparing models is useful when we want to estimate the variance explained by a set of effects <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span>, and test this variance. For instance, one may have an experiment with a series of possible confounding variables, and the aim is to estimate the variance explained by the experimental factors (all together) over and beyond the confounding variables explained variances. Another case may be a model in which socio-economical variables (e.g income, real-estate properties, etc) and psychological variables (e.g self-esteem, emotional regulations) are compared in their ability to explain an outcome (say <em>happyness</em>). Besides each individual variable effect size, the researcher may be interested in estimating the variance explained by <em>economics</em> vs the variance explained by <em>psychology</em>. A model-comparison approach may be helpful.</p>
<p>In our running example, we would like to estimate and test the impact of <em>beer</em> (<span class="math inline">\(x\)</span>) and <em>extraversion</em> (<span class="math inline">\(z\)</span>) over and beyond the effect of <em>type of bar</em> (<span class="math inline">\(cat2\)</span>) and <em>type of beer</em> (<span class="math inline">\(cat3\)</span>). Our full model involves all mentioned variables, plus the interaction between the factors (see Figure above). We now need to specify a smaller model, which includes only the categorical variables and their interaction. We set this model in the <span class="option">Nested Model</span> field.</p>
<p><img src="bookletpics/2_modelcomparison_input2.png" width="570" /></p>
<p>What we are saying to the module is to estimate a full model, then estimate a model with only the categorical variables, and compare the fit (<span class="math inline">\(R^2\)</span>). The difference between the two <span class="math inline">\(R^2\)</span>’s is the variance uniquely explained by the terms present only in the full model, in our case <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>. The output is the following:</p>
<p><img src="bookletpics/2_modelcomparison_output2.png" width="495" /></p>
<p>Thus, the full model explains <span class="math inline">\(R^2=.406\)</span> (*100) of the variance of the dependent variable, the Nested model, without <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> explains <span class="math inline">\(R^2=.162\)</span> (*100) of the variance. Their difference, <span class="math inline">\(\Delta R^2=.245\)</span> is the variance uniquely due to <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> together.</p>
</div>
<div id="types-of-tests" class="section level3 hasAnchor" number="2.14.2">
<h3><span class="header-section-number">2.14.2</span> Types of tests<a href="glm.html#types-of-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the General Linear Model, all tests are usually performed with F-test. The F-test works fine and has a solid tradition in statistics. Therefore, it is perfectly fine to use it also to compare models. After all, model-comparison entails comparing variances explained, which is the F-test job since more than 100 years. Recently, the statistical literature has made another test popular in model-comparison methods: The LRT, log-likelihood ratio test. This test is very useful for models estimated maximizing the log-likelihood of the data given the model, such as the generalized linear model or (with some caveats), the mixed model. For those passionate about the LRT, <span class="modulename">GAMLj</span> offers the option to obtain the LRT also for the GLM, by selecting <span class="option">LRT</span> under <span class="option">Omnibus test</span>. In the GLM, the p-values based on the F-test and the LRT are usually very similar.</p>
<p><img src="bookletpics/2_modelcomparison_input3.png" width="559" />
<img src="bookletpics/2_modelcomparison_output3.png" width="451" /></p>
</div>
<div id="not-necessary-model-comparisons" class="section level3 hasAnchor" number="2.14.3">
<h3><span class="header-section-number">2.14.3</span> Not necessary model-comparisons<a href="glm.html#not-necessary-model-comparisons" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One should not be carried away by model-comparisons (the custom version of the method), because it is seldom useful and the <em>usual</em> estimates and tests of the linear model are already some sort of model-comparisons. Recall, for instance, the model with <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> as independent variables.</p>
<p><img src="bookletpics/2_modelcomparison_input4.png" width="559" />
<img src="bookletpics/2_modelcomparison_output4.png" width="510" /></p>
<p>Notice that we selected <span class="math inline">\(\eta^2\)</span> as an additional effect size index. First, if we make a model-comparison with an intercept only model, we get exactly the <span class="math inline">\(R^2\)</span> we obtained in the standard analysis. This is because the <em>usual</em> <span class="math inline">\(R^2\)</span> is already a fit comparison between our full model and an intercept-only model (cf appendix <a href="appendixa.html#appendixa">A</a>).</p>
<p><img src="bookletpics/2_modelcomparison_input5.png" width="555" />
<img src="bookletpics/2_modelcomparison_output5.png" width="514" /></p>
<p>Second, if we now add to the nested model the variable <span class="math inline">\(z\)</span>, we are comparing a model with <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> as terms with a model with only <span class="math inline">\(z\)</span>, so we are estimating the contribution of <span class="math inline">\(x\)</span> to the explained variance.
<img src="bookletpics/2_modelcomparison_input6.png" width="560" />
<img src="bookletpics/2_modelcomparison_output6.png" width="570" /></p>
<p>The <span class="math inline">\(\Delta R^2\)</span> is now .317, with <code>F(1,117)=56.68</code>, which is exactly the <span class="math inline">\(\eta^2\)</span> of the <span class="math inline">\(x\)</span> in the full model, with the same F, df, and p-value. The effect of <span class="math inline">\(x\)</span> is already a result of a model-comparison test, so we do not need to test it explicitly.</p>
</div>
<div id="hierarchical-regression" class="section level3 hasAnchor" number="2.14.4">
<h3><span class="header-section-number">2.14.4</span> Hierarchical regression<a href="glm.html#hierarchical-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model-comparison approach allows estimating what many people call <em>hierarchical regression</em>. Hierarchical regression is an analytic strategy, it is not a statistical model. By <em>hierarchical regression</em> one means the estimation of the coefficients of different <span class="tooltip">IV <span class="tooltiptext">Independent Variable</span> </span> in different models, containing different sets of <span class="tooltip">IV <span class="tooltiptext">Independent Variable</span> </span>. For instance, one may want to estimate the effects of <span class="math inline">\(cat2\)</span> and <span class="math inline">\(cat3\)</span> independently of <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>, but the effects of <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> keeping constant <span class="math inline">\(cat2\)</span> and <span class="math inline">\(cat3\)</span>. In a hierachical regression software, one specify a first block with <span class="math inline">\(cat2\)</span> and <span class="math inline">\(cat3\)</span>, and a second block with all variables. The software would estimate two models, and produce a recap table with the coefficients obtained in the first model for <span class="math inline">\(cat2\)</span> and <span class="math inline">\(cat3\)</span>, and the results obtained in the second model for <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>. Possibly, the <span class="math inline">\(\Delta R^2\)</span> is also produced.</p>
<p>This analytic strategy is rarely useful, but if this is really the intent of the analyst, one can use <span class="modulename">GAMLj</span> to estimate two different models to obtain the coefficients, and then obtain the <span class="math inline">\(\Delta R^2\)</span> with a model-comparison approach. Alternatively, one can use <span class="jamovi"><a href="http://www.jamovi.org" target="_blank">jamovi</a></span> <code>regression</code> command, which allows to specify blocks. Results will be identical, only the tables will be organized in different ways.</p>
</div>
</div>
<div id="assumptions-checks" class="section level2 hasAnchor" number="2.15">
<h2><span class="header-section-number">2.15</span> Assumptions checks<a href="glm.html#assumptions-checks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The GLM is based on several assumptions, a few of which are popular and they are regularly tested or at least checked. We should start saying that assumptions are idealized scenarios in which data show a required property. They are needed to unsure that the expected results (F-test, p-values, etc.) posses the expected properties (unbiasedness, consistency, etc). In other words, the assumptions are required so we can trust the results, cf. <span class="citation">Nimon (<a href="#ref-nimon" role="doc-biblioref">2012</a>)</span> and <span class="citation">Glass, Peckham, and Sanders (<a href="#ref-glass1972consequences" role="doc-biblioref">1972</a>)</span> for details.</p>
<p>Being idealized scenarios, the observed data are never perfectly abiding by the assumptions, but they approximate the required property with different degrees. The better a property is approximated, the more we can trust the results. The worse is approximated, the more doubts we should cast on our results. Because the properties required by the assumptions are only approximated to a certain degree, assumptions cannot be evaluated only using an inferential test. Inferential tests tend to be interpret as “significant” vs “not significant”, and such a dichotomy is not always useful when evaluating assumptions. For this reason, <span class="modulename">GAMLj</span> provides both inferential tests and graphical methods to assess the appropriateness of the data with respect of the assumptions.</p>
<p>Here we focus on the <a href='https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity' target='_blank'>homoschedasticity</a> and <a href='https://online.stat.psu.edu/stat462/node/122/' target='_blank'>normality of residuals</a> assumptions.</p>
<div id="homosched" class="section level3 hasAnchor" number="2.15.1">
<h3><span class="header-section-number">2.15.1</span> Homoschedasticity<a href="glm.html#homosched" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By <em>Homoschedasticity</em> we mean that the variance of the residuals is constant around the predicted values, that this the residuals spread around the predicted values at more or less the same distance along the model predictions. In other words, the spread of the clouds of points representing the <span class="tooltip">DV <span class="tooltiptext">Dependent Variable</span> </span> as a function of the <span class="tooltip">IV <span class="tooltiptext">Independent Variable</span> </span> is constant. When the spread is not constant, we have <em>Heteroschedasticity</em>. Here are two exemplifications:</p>
<p><img src="bookletpics/2_assumptions_theory1.png" width="464" /></p>
<p>In cases where the independent variables are categorical, the assumption requires that the variances within groups are more or less the same across groups.</p>
<p>The idea is that the error term, which is the residuals variance, should be representative of the residual variance across all values of the predicted values. This is <em>Homoschedasticity</em>. On the contrary, the error term is not representative of the whole model if the variance of the resifuals is different for different predicted values, because it would be larger or smaller in different parts of the model. That is <em>Heteroschedasticity</em>. <em>Homoschedasticity</em> ensures that the standard errors associated with the estimates are correct, and thus are the inferential tests and the p-values one obtains along the coefficients estimates.</p>
<p>To evaluate the <em>Homoschedasticity</em> we can use two inferential tests and one graphical method.</p>
<p><img src="bookletpics/2_assumptions_input1.png" width="554" /></p>
<p>The tests are the <a href='https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test' target='_blank'>Breusch-Pagan test</a> and the <a href='https://en.wikipedia.org/wiki/Levene%27s_test' target='_blank'>Levene’s test</a>.</p>
<p><img src="bookletpics/2_assumptions_output1.png" width="475" /></p>
<p>Both tests test the null-hypothesis that the variance of the residuals do not change along the predicted values, so the assumption is met when they are not significant. A significant test, on the contrary, indicates that the residuals variance departs from the assumption of homogeneity, and thus we should cast some doubt on the validity of the results.</p>
<p>The Breusch-Pagan is defined for any GLM, so it is estimated whatever our <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span> are. The Levene’s test is defined only for categorical <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span>, so it is not produced when the <span class="tooltip">IVs <span class="tooltiptext">Independent Variables</span> </span> are all continuous.</p>
<p>Graphically, we can check the <span class="option">Residuals-Predicted plot</span>. This plot depicts the residuals (in the Y-axis) as a function of the predicted (on the X-axis), so it makes it easy to see whether the spread of the cloud of points changes along the X-axis. The assumption is approximated well when the depicted cloud of points has more or less the same spread along the whole plot. Examples may be:</p>
<p><img src="bookletpics/2_assumptions_output2.png" width="511" /></p>
</div>
<div id="normality-of-residuals" class="section level3 hasAnchor" number="2.15.2">
<h3><span class="header-section-number">2.15.2</span> Normality of residuals<a href="glm.html#normality-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To guarantee unbiased inferential tests, that is valid p-values, residuals should be <a href='https://en.wikipedia.org/wiki/Normal_distribution' target='_blank'>normally distributed (Gaussian)</a>. <span class="modulename">GAMLj</span> provides the <a href='https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test' target='_blank'>Kolmogorov-Smirnov test</a>, the <a href='https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test' target='_blank'>Shapiro-Wilk test</a>, the histogram plot and the Q-Q plot to assess this assumption.</p>
<p><img src="bookletpics/2_assumptions_input3.png" width="561" /></p>
<p><img src="bookletpics/2_assumptions_output3.png" width="357" /></p>
<p>Both inferential tests share the null-hypothesis that the distribution is normal (Gaussian), thus a non-significant test indicates lack of evidence against the assumption, whereas a significant test indicates departure from the assumptions. Shapiro-Wilk seems more powerful, so it should be preferred in small samples, whereas the Kolmogorov-Smirnov may be more appropriate in large samples.</p>
<p>The first graphical method is a simple histogram of the residuals, on which the module overlays a perfectly normal curve with the same mean and standard deviation of the observed one, so the comparison becomes easier.</p>
<p><img src="bookletpics/2_assumptions_output4.png" width="527" /></p>
<p>Here we want to check that our distribution is not too far away from a normal distribution, especially with regards of the main properties of the normal one: symmetry and increasing density (frequency of cases) closer to the mean.</p>
<p>Another popular graphical method is the Q-Q plot.</p>
<p><img src="bookletpics/2_assumptions_output5.png" width="582" /></p>
<p>The Q-Q plot plots the theoretical quantiles (percentiles) of a perfectly normally distributed variable with the quantiles of our observed distribution. The closer our distribution to the normal one, the closer the scattered points will be to the 45 degrees line.</p>
</div>
</div>
<div id="violations-remedies" class="section level2 hasAnchor" number="2.16">
<h2><span class="header-section-number">2.16</span> Violations Remedies<a href="glm.html#violations-remedies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><img src="bookletpics/2_options_input1.png" width="572" /></p>
<div id="robust-standard-error" class="section level3 hasAnchor" number="2.16.1">
<h3><span class="header-section-number">2.16.1</span> Robust Standard Error<a href="glm.html#robust-standard-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Whereas it is often the case that both assumptions are violated, the violation of <em>homoschedasticity</em> and the violation of <em>normality of the residuals</em> have different remedies.</p>
<p>Lack of <em>homoschedasticity</em>, that is <em>heteroschedasticity</em>, can be counteracted by using a robust method to compute the standard errors. Whereas <em>robust</em> estimations is a very general term, in the context of the GLM it usually means <em>robust against heteroschedasticity</em>. <span class="modulename">GAMLj</span> offers <em>robust</em> standard error in the <span class="option">Options</span> panel, under <span class="option">SE method</span>. When <code>Robust</code> is selected, one can choose the algorithm to compute the HC (<strong>H</strong>eteschadasticity-<strong>C</strong>onsistent) standard errors. <span class="modulename">GAMLj</span> implements these algorithms as implemented in the <a href='https://cran.r-project.org/web/packages/sandwich/sandwich.pdf' target='_blank'>sandwich R package</a>, setting <code>HC3</code> as default as recommended by the package authors.</p>
<p>Setting the SE method to robust updates all results related with inferential tests, and thus the results will be different, and more accurate, proportionally to the strength of the assumption violation.</p>
</div>
<div id="bootstrap-confidence-intervals" class="section level3 hasAnchor" number="2.16.2">
<h3><span class="header-section-number">2.16.2</span> Bootstrap Confidence Intervals<a href="glm.html#bootstrap-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the residuals are not normally distributed, or when other assumptions are suspicious,
one can rely on <a href='https://elizavetalebedeva.com/bootstrapping-confidence-intervals-the-basics/' target='_blank'>Bootstrap Confidence Intervals</a>. The advantage of the bootstrap C.I. is that they do not assume any shape for the residual distribution, because their boundaries are estimated by resampling the observed distribution (cf. <a href='https://en.wikipedia.org/wiki/Bootstrapping_(statistics)' target='_blank'>Bootstrapping</a>, and <span class="citation">Efron and Tibshirani (<a href="#ref-efron1994introduction" role="doc-biblioref">1994</a>)</span>).</p>
<p><span class="modulename">GAMLj</span> offers two methods to compute bootstrap confidence intervals. The <code>percent</code> and the <code>BCa</code> method. The <code>percent</code> method entails building a bootstrap distribution of estimates and select the <span class="math inline">\(100 \cdot (\alpha/2)\)</span>th and the <span class="math inline">\(100 \cdot (1-{\alpha /2})\)</span>th percentile of the distribution as the boundary of the interval. In other words, for the 95% C.I (where <span class="math inline">\(\alpha=.05\)</span>), it selects the 2.5th and 97.5th percentile of the bootstrap distribution. The <code>BCa</code> method stands for <em>Bias corrected and accelerated</em> method. The method corrects the percentile values depending on the skewness of the bootstrap estimates distribution and the offset of the distribution mean compared with the observed estimate. The specialized literature seems to indicate that for generic situations, the <em>percent</em> method gives more accurate results (cf., for instance, <span class="citation">Jung et al. (<a href="#ref-jung2019comparison" role="doc-biblioref">2019</a>)</span>).</p>
<p>The number of bootstrap resampling is set to 1000, but for reliable and replicable results one can set it to 5000 or 10000. Just keep in mind that resampling means to estimate a full model for every bootstrap sample, so the process might be quite time-consuming for large models.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-aiken1991multiple" class="csl-entry">
Aiken, Leona S, Stephen G West, and Raymond R Reno. 1991. <em>Multiple Regression: Testing and Interpreting Interactions</em>. sage.
</div>
<div id="ref-cohen2013statistical" class="csl-entry">
Cohen, Jacob. 2013. <em>Statistical Power Analysis for the Behavioral Sciences</em>. Academic press.
</div>
<div id="ref-cohen2014applied" class="csl-entry">
Cohen, Patricia, Stephen G West, and Leona S Aiken. 2014. <em>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences</em>. Psychology press.
</div>
<div id="ref-efron1994introduction" class="csl-entry">
Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.
</div>
<div id="ref-glass1972consequences" class="csl-entry">
Glass, Gene V, Percy D Peckham, and James R Sanders. 1972. <span>“Consequences of Failure to Meet Assumptions Underlying the Fixed Effects Analyses of Variance and Covariance.”</span> <em>Review of Educational Research</em> 42 (3): 237–88.
</div>
<div id="ref-hedges2014statistical" class="csl-entry">
Hedges, Larry V, and Ingram Olkin. 2014. <em>Statistical Methods for Meta-Analysis</em>. Academic press.
</div>
<div id="ref-judd2017data" class="csl-entry">
Judd, Charles M, Gary H McClelland, and Carey S Ryan. 2017. <em>Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond</em>. Routledge.
</div>
<div id="ref-jung2019comparison" class="csl-entry">
Jung, Kwanghee, Jaehoon Lee, Vibhuti Gupta, and Gyeongcheol Cho. 2019. <span>“Comparison of Bootstrap Confidence Interval Methods for GSCA Using a Monte Carlo Simulation.”</span> <em>Frontiers in Psychology</em> 10: 2215.
</div>
<div id="ref-mayoci" class="csl-entry">
Mayo, Deborah G. 1981. <span>“In Defense of the Neyman-Pearson Theory of Confidence Intervals.”</span> <em>Philosophy of Science</em> 48 (2): 269–80. <a href="https://doi.org/10.1086/288996">https://doi.org/10.1086/288996</a>.
</div>
<div id="ref-midway2020comparing" class="csl-entry">
Midway, Stephen, Matthew Robertson, Shane Flinn, and Michael Kaller. 2020. <span>“Comparing Multiple Comparisons: Practical Guidance for Choosing the Best Multiple Comparisons Test.”</span> <em>PeerJ</em> 8: e10387.
</div>
<div id="ref-nimon" class="csl-entry">
Nimon, Kim. 2012. <span>“Statistical Assumptions of Substantive Analyses Across the General Linear Model: A Mini-Review.”</span> <em>Frontiers in Psychology</em> 3. <a href="https://doi.org/10.3389/fpsyg.2012.00322">https://doi.org/10.3389/fpsyg.2012.00322</a>.
</div>
<div id="ref-olejnik2003generalized" class="csl-entry">
Olejnik, Stephen, and James Algina. 2003. <span>“Generalized Eta and Omega Squared Statistics: Measures of Effect Size for Some Common Research Designs.”</span> <em>Psychological Methods</em> 8 (4): 434.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>I have nothing against zero: my favorite number is 610, which in Italian literally translates to <em>you are a zero</em>, where “you” is meant to be “we all”<a href="glm.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="booklet.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gzlm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gamljmodels.pdf", "gamljmodels.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
