<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Details: GLM effect size indices</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KM7X6HK5HM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KM7X6HK5HM');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="helpstyle.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">GAMLj</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="glm.html">GLM</a>
</li>
<li>
  <a href="mixed.html">Mixed</a>
</li>
<li>
  <a href="gzlm.html">Generalized Models</a>
</li>
<li>
  <a href="gzlmmixed.html">Generalized Mixed Models</a>
</li>
<li>
  <a href="examples.html">Examples and Details</a>
</li>
<li>
  <a href="release_notes.html">Release notes</a>
</li>
<li>
  <a href="vignettes.html">GAMLj in R</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/gamlj/gamlj">View on Github</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Details: GLM effect size indices</h1>

</div>


<p><span class="version"> <span class="versiontitle"> GAMLj version ≥
</span> 2.6.1 </span></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Standardized Effect size indices produced by GLM module are the
following:</p>
<ul>
<li><span class="math inline">\(\beta\)</span> : standardized regression
coefficients</li>
<li><span class="math inline">\(\eta^2\)</span>: (semi-partial)
eta-squared</li>
<li><span class="math inline">\(\eta^2\)</span>p : partial
eta-squared</li>
<li><span class="math inline">\(\omega^2\)</span> : omega-squared</li>
<li><span class="math inline">\(\omega^2\)</span>p : partial
omega-squared</li>
<li><span class="math inline">\(\epsilon^2\)</span> :
epsilon-squared</li>
<li><span class="math inline">\(\epsilon^2\)</span>p : partial
epsilon-squared</li>
</ul>
<p>All coefficients but the betas are computed with the approapriate
function of the R package <a
href="https://cran.r-project.org/web/packages/effectsize/index.html">effectsize</a>,
with some adjustment.</p>
</div>
<div id="beta-beta" class="section level1">
<h1><span class="math inline">\(\beta\)</span> : beta</h1>
<p>For continuous variables, it simply corresponds to the B coefficient
obtained after standardizing all variables in the model. The
standardization of the continuous variables is done before any
transformation is applied, so if a complex model requires interaction or
polynomial terms, the terms are computed after standardization, and the
<span class="math inline">\(\beta\)</span> are consistent.</p>
<p>For categorical variables, however, some comments are in order:
Categorical variables are not standardized in <span
class="modulename">GAMLj</span>, so the <span
class="math inline">\(\beta\)</span> should be interpreted in terms of
standardized differences in the dependent variable between the levels
contrasted by the corresponding contrast. Consider the following
example: Two groups (variable <code>groups</code>) of size 20 and 10
respectively, are compared on a variable Y. If one uses <span
class="modulename">GAMLj</span> default contrast coding
(<code>simple</code>), the B is the difference in groups means. The
<span class="math inline">\(\beta\)</span> is the difference between the
average z-scores of the dependent variable between the two groups.
Assume these are the results:</p>
<p><img src="details/glm/effectsize_example1_1.png" width="717" /></p>
<p>The beta is 0.352, so it means that if we compute the mean difference
between groups in the standardized <em>y</em>, we obtain 0.352. In
fact.</p>
<p><img src="details/glm/effectsize_example1_2.png" width="558" /></p>
<p>However, the <span class="math inline">\(\beta\)</span> you obtain is
not the correlation between <em>zy</em> and <em>groups</em>. The
correlation is 0.169:</p>
<p><img src="details/glm/effectsize_example1_3.png" width="413" /></p>
<p>Why is there this discrepancy? Because the groups are not balanced,
so when the correlation is computed, the variable <em>groups</em> is
standardized, so the contrast coding values depend on the relative size
of the groups. The actual groups coding values used by the Pearson’s
correlations are the following:</p>
<p><img src="details/glm/effectsize_example1_4.png" width="340" /></p>
<p>Thus, the correlation corresponds to running a regression with
<em>zy</em> as dependent variables and a continuous variable featuring
either -.695 or 1.390 as values. The <span
class="math inline">\(\beta\)</span> yielded by <span
class="modulename">GAMLj</span>, instead, is the mean difference between
X levels on the standardized Y. Please notice that other software may
yield different <span class="math inline">\(\beta\)</span>’s for
categorical variables.</p>
<p>If two groups are balanced and homeschedastic, the <span
class="math inline">\(\beta\)</span> associated with a
<code>deviation</code> contrast corresponds to the fully standardized
coefficient.</p>
</div>
<div id="eta2-semi-partial-eta-squared" class="section level1">
<h1><span class="math inline">\(\eta^2\)</span>: (semi-partial)
eta-squared</h1>
<p>This is the proportion of total variance uniquely explained by the
associated effect. Being <span class="math inline">\(SS_{eff}\)</span>
the sum of squares of the effect, <span
class="math inline">\(SS_{res}\)</span> the sum of squares of the
residuals or of SS error, and <span
class="math inline">\(SS_{model}\)</span> the sum of sum of squares of
the whole model, we have:</p>
<p><span class="math display">\[\eta^2={{SS_{eff} \over
{SS_{model}+SS_{res}}}}\]</span></p>
<p>where <span
class="math inline">\(SS_{model}+SS_{res}=SS_{total}\)</span> and <span
class="math inline">\(SS_{total}=\sum(y_i-\bar{y})^2\)</span> and <span
class="math inline">\(SS_{model}=\sum(\hat{y_i}-\bar{\hat{y}})^2\)</span>.</p>
<p>Please notice that although the computation of the effect size
indexes and their confidence intervals is carried out employing <a
href="https://github.com/easystats/effectsize">effectsize R package</a>,
<span class="modulename">GAMLj</span> makes a correction to the
computation of <span class="math inline">\(\eta^2\)</span> and of the
other non-partial indices. <code>effectsize</code> R package, infact,
defines the total sum of squares as <span
class="math inline">\(SS_{total}^*=\sum{SS_{f}+SS_{res}}\)</span>, where
<span class="math inline">\(f\)</span> is any effect in the model. For
balanced designs and many other models, <span
class="math inline">\(SS_{total}^*=SS_{total}\)</span>, so no issue
arises. However, there are certain models in which <span
class="math inline">\(SS_{total}^*\ne SS_{total}\)</span>, and so the
index looses some of its properties when computed based on <span
class="math inline">\(SS_{total}^*\)</span>. <span
class="modulename">GAMLj</span> operates a correction such that all the
non-partial indeces are always computed based on <span
class="math inline">\(SS_{total}=SS_{model}+SS_{res}\)</span>.</p>
<button type="button" class="collapsible">
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;">
<path d="M344 0H488c13.3 0 24 10.7 24 24V168c0 9.7-5.8 18.5-14.8 22.2s-19.3 1.7-26.2-5.2l-39-39-87 87c-9.4 9.4-24.6 9.4-33.9 0l-32-32c-9.4-9.4-9.4-24.6 0-33.9l87-87L327 41c-6.9-6.9-8.9-17.2-5.2-26.2S334.3 0 344 0zM168 512H24c-13.3 0-24-10.7-24-24V344c0-9.7 5.8-18.5 14.8-22.2s19.3-1.7 26.2 5.2l39 39 87-87c9.4-9.4 24.6-9.4 33.9 0l32 32c9.4 9.4 9.4 24.6 0 33.9l-87 87 39 39c6.9 6.9 8.9 17.2 5.2 26.2s-12.5 14.8-22.2 14.8z"/>
</svg>
<span class="colltitle"> Why this correction? </span> <span
class="collinfo">click to read</span> <span class="collinfo"> technical
stuff </span>
</button>
<div class="collapsiblecontent">
<p>With the correction, one property <span
class="math inline">\(\eta^2\)</span> retains even when <span
class="math inline">\(SS_{total}^*\ne SS_{total}\)</span> is that <span
class="math inline">\(\eta^2=r_{sp}^2\)</span>, where <span
class="math inline">\(r_{sp}\)</span> is the semi-partial correlation
<span class="citation">(<a href="#ref-cohen2014applied">Cohen, West, and
Aiken 2014</a>)</span>. We can use the <code>exercise</code> dataset
from <span class="citation">(<a href="#ref-cohen2014applied">Cohen,
West, and Aiken 2014</a>)</span> to see this in practice (refer to
<a href="glm_example1.html">GLM: Multiple regression, moderated
regression, and simple slopes</a> for a complete analysis). If we run a
multiple regression <code>yendu~xage+zexer</code> and ask for the <span
class="math inline">\(\eta^2\)</span>’s, we obtain the following
results:</p>
<p><img src="details/glm/eta1.png" class="img-responsive" alt=""></p>
<p>Going in <span
class="jamovi"><a href="http://www.jamovi.org" target="_blank">jamovi</a></span>,
<span class="option">Regression-&gt;Partial correlation</span> we can
compute the semi-partial correlation of each IV coviariating the other.
This gives:</p>
<p><img src="details/glm/eta2.png" class="img-responsive" alt=""></p>
<p>Squaring it gives <span
class="math inline">\(r_{yx.z}^2=-0.230^2=0.0529\)</span>, which is
equal to the corrisponding <span
class="math inline">\(\eta^2=.053\)</span>.</p>
<p><img src="details/glm/eta3.png" class="img-responsive" alt=""></p>
<p>Squaring the second <span class="math inline">\(r_{sp}^2\)</span>
gives <span class="math inline">\(r_{yz.x}^2=0.388^2=0.1504664\)</span>,
considering rounding, which is equal to the corrisponding <span
class="math inline">\(\eta^2=.150\)</span>.</p>
<p>If we used <span
class="math inline">\(SS_{total}^*=\sum{SS_{f}}+SS_{res}\)</span>,
results would be:</p>
<ul>
<li><span
class="math inline">\(SS_{total}^*=1516+4298+23810=29624\)</span></li>
<li><span class="math inline">\(SS_{yx.z}^*=1516/29624
=0.0511747\)</span></li>
<li><span class="math inline">\(SS_{yz.x}^*=4298/29624
=0.1450851\)</span></li>
</ul>
<p>that are clearly not corresponding to the <span
class="math inline">\(r_{sp}^2\)</span>, as they should be. We should
note, however, that the two methods of estimation usually give very
similar results, even when <span class="math inline">\(SS_{total}^* \ne
SS_{total}\)</span> and exactly the same results when <span
class="math inline">\(SS_{total}^*=SS_{total}\)</span>.</p>
<p>The same reasoning holds for all the non-partial indices.</p>
</div>
<button type="button" class="collapsible">
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;">
<path d="M344 0H488c13.3 0 24 10.7 24 24V168c0 9.7-5.8 18.5-14.8 22.2s-19.3 1.7-26.2-5.2l-39-39-87 87c-9.4 9.4-24.6 9.4-33.9 0l-32-32c-9.4-9.4-9.4-24.6 0-33.9l87-87L327 41c-6.9-6.9-8.9-17.2-5.2-26.2S334.3 0 344 0zM168 512H24c-13.3 0-24-10.7-24-24V344c0-9.7 5.8-18.5 14.8-22.2s19.3-1.7 26.2 5.2l39 39 87-87c9.4-9.4 24.6-9.4 33.9 0l32 32c9.4 9.4 9.4 24.6 0 33.9l-87 87 39 39c6.9 6.9 8.9 17.2 5.2 26.2s-12.5 14.8-22.2 14.8z"/>
</svg>
<span class="colltitle"> Isn’t this weird? </span> <span
class="collinfo">click to read</span> <span class="collinfo"> technical
stuff </span>
</button>
<div class="collapsiblecontent">
<p>For many people, the fact that <span
class="math inline">\(SS_{model}^* \ne SS_{model}\)</span> may come as a
surprise. Maybe because in the ANOVA tradition, with balanced designs,
<span class="math inline">\(\sum{SS_{f}}\)</span> is always equal to
<span class="math inline">\(SS_{model}\)</span>, or maybe because it
would be nicer if it was always like in the ANOVA. Since we have to
accept that in the GLM <span class="math inline">\(\sum{SS_{f}}\)</span>
is not necessarily equal to <span
class="math inline">\(SS_{model}\)</span>, let us see when this
happens.</p>
<p>There are two cases: The easiest to understand is when <span
class="math inline">\(\sum{SS_{f}} \lt SS_{model}\)</span>. This case
happens when the independent variables are positively (or all
negatively) correlated so each variable explains a unique part of the
variance, but the model sum of square involves also some shared
variance, which ends up in <span
class="math inline">\(SS_{model}\)</span> but not in <span
class="math inline">\(\sum{SS_{f}}\)</span>.</p>
<p>A little trickier is the case when <span
class="math inline">\(\sum{SS_{f}} \gt SS_{model}\)</span>, because it
seems strange that the sum of effects is larger than the overall
combined effect.</p>
<p>Consider a regression <span
class="math inline">\(y=a+b_{yx.z}x+b_{yz.x}z\)</span>. Recall the the
<span class="math inline">\(SS_{yx.z}\)</span> (<span
class="math inline">\(x\)</span> explains <span
class="math inline">\(y\)</span> keeping constant <span
class="math inline">\(z\)</span>) is computed as <span
class="math inline">\(SS_{model}-SS_{yz}\)</span>, where <span
class="math inline">\(SS_{yz}\)</span> is the sum of squares explained
by <span class="math inline">\(z\)</span> without <span
class="math inline">\(x\)</span> in the model, and <span
class="math inline">\(SS_{yz.x}=SS_{model}-SS_{yx}\)</span>. It follows
that the <span class="math inline">\(\sum{SS_f}=2\cdot
SS_{model}-SS_{yx}-SS_{yz}\)</span> is larger than <span
class="math inline">\(SS_{model}\)</span> when <span
class="math inline">\(SS_{model} &gt; SS_{yx}+SS_{yz}\)</span>. This
necessarily means that at least one variable explains more variance
while keeping constant the other than alone.</p>
<p>Indeed, in the example above about exercising, the SS of <em>age</em>
alone is 452, and <em>exer</em> alone is 3234, which sum to 3686, less
than 4751, the multiple regression model SS.</p>
<p>The question is now: when does this happen? Well, it happens when
<span class="math inline">\(B_{yz}\)</span>, the coefficient associated
with <span class="math inline">\(z\)</span> in simple regression, is
smaller (in absolute value) than the partial coefficient of <span
class="math inline">\(B_{yz.x}\)</span> of a multiple regression.
Because <span class="math inline">\(B_{yz.x}=B_{yz} - B_{yx.z}\cdot
B_{xz}\)</span>, this happens when <span
class="math inline">\(B_{yz}\)</span> and <span
class="math inline">\(B_{yx.z}\cdot B_{xz}\)</span> have different
signs: therefore, we have a suppression effect!</p>
<p><img src="details/glm/simplezony.png" class="img-responsive" alt="">
<img src="details/glm/simplezonx.png" class="img-responsive" alt="">
<img src="details/glm/mutiplezxony.png" class="img-responsive" alt=""></p>
<p>In the example above, focusing on <em>exer</em> (the z variable), we
have <span class="math inline">\(B_{yz.x}=.916\)</span>, <span
class="math inline">\(B_{yx.z}=-.257\)</span> and <span
class="math inline">\(B_{xz}=.598\)</span>, thus <span
class="math inline">\(B_{yx.z}\cdot
B_{xz}=-.257*.598=-0.153686\)</span>, which has a different sign than
<span class="math inline">\(B_{yz}=.762\)</span>. Thus, the effect of
<em>exercising</em> is stronger when keeping constant <em>age</em> than
alone: <em>age</em> suppresses the effect of <em>exercising</em> on
<em>endurance</em>.</p>
</div>
</div>
<div id="eta2p-partial-eta-squared" class="section level1">
<h1><span class="math inline">\(\eta^2\)</span>p : partial
eta-squared</h1>
<p>This is the proportion of partial variance uniquely explained by the
associated effect. That is, the variance uniquely explained by the
effect expressed as the proportion of variance not explained by the
other effects. Here the variance explained by the other effects in the
model is completely partialed out. Its formula is:</p>
<p><span class="math display">\[\eta^2p={{SS_{eff} \over
{SS_{eff}+SS_{res}}}}\]</span></p>
<p>clearly, if there is only one independent variable, <span
class="math inline">\(\eta^2=\eta^2p\)</span></p>
</div>
<div id="omega2-omega-squared" class="section level1">
<h1><span class="math inline">\(\omega^2\)</span> : omega-squared</h1>
<p>This is the <em>expected value in the population</em> of the
proportion of variance uniquely explained by the associated effect. In
other words, it is the unbiased version of <span
class="math inline">\(\eta^2\)</span>. There are different formulas to
visualize its computation, here is one. If <span
class="math inline">\(df_{res}\)</span> are the degrees of freedon of
the residual variance, <span class="math inline">\(df_{eff}\)</span> are
the degrees of freedom of the effect, we have:</p>
<p><span class="math display">\[\omega^2={{SS_{eff}-SS_{res} \cdot
({df_{eff}/df_{res}) \cdot }}\over{
SS_{model}+SS_{res}(df_{res}+1)/df_{res}}}\]</span></p>
<p>It’s clear that omega is similat to <span
class="math inline">\(\eta^2\)</span>, but applies a correction for the
denominator.</p>
</div>
<div id="omega2p-partial-omega-squared" class="section level1">
<h1><span class="math inline">\(\omega^2\)</span>p : partial
omega-squared</h1>
<p>This is the <em>expected value in the population</em> of the
proportion of <em>partial</em> variance uniquely explained by the
associated effect. In other words,it is the unbiased version of <span
class="math inline">\(\eta^2p\)</span>. With N being the sample size, We
have:</p>
<p><span class="math display">\[\omega^2p={{SS_{eff}-SS_{res} \cdot
({df_{eff}/df_{res}) \cdot }}\over{ SS_{eff}+SS_{res} \cdot
[{(N-df_{eff})/df_{res}}]
}}\]</span></p>
<p>It’s clear that omega is similat to <span
class="math inline">\(\eta^2p\)</span>, but applies a correction for the
degress of freedom. In fact, as N increases, the two indices
converge.</p>
</div>
<div id="epsilon2p-epsilon-squared" class="section level1">
<h1><span class="math inline">\(\epsilon^2\)</span>p :
epsilon-squared</h1>
<p>Epsilon-squared is another correction of <span
class="math inline">\(\eta^2\)</span>, but the correction involves only
the estimation of the sum of squares of the effect, not the partial
variance on which the effect is compared</p>
<p><span class="math display">\[\epsilon^2={{SS_{eff}-SS_{res} \cdot
({df_{eff}/df_{res}) \cdot }}\over{ SS_{model}+SS_{res}}}\]</span></p>
</div>
<div id="epsilon2p-partial-epsilon-squared" class="section level1">
<h1><span class="math inline">\(\epsilon^2\)</span>p : partial
epsilon-squared</h1>
<p>As for the non-partial Epsilon, the partial Epsilon-squared is a
correction of <span class="math inline">\(\eta^2p\)</span>, but the
correction involves only the estimation of the sum of squares of the
effect, not the partial variance on which the effect is compared</p>
<p><span class="math display">\[\epsilon^2p={{SS_{eff}-SS_{res} \cdot
({df_{eff}/df_{res}) \cdot }}\over{ SS_{eff}+SS_{res}}}\]</span></p>
</div>
<div id="simple-effects" class="section level1">
<h1>Simple Effects</h1>
<p>From version 2.6.1 on, all the effect size indices are available also
for the simple effects. To compute them, <span
class="modulename">GAMLj</span> extracts the SS of the simple effect
from <code>R emmeans</code> F-tests. The SS residuals and SS model is
extracted from the model summary, given that both SS do not change when
simple effects are computed. Then the indices are computed using the
previously described formulas.</p>
<p>In particular, if the simple effect is <span
class="math inline">\(se\)</span>: <span
class="math display">\[SS_{res}=\sigma^2\cdot df_{res}\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is extracted as
<code>sigma(model)</code>.</p>
<p><span class="math display">\[SS_{model}={{F_{model}\cdot df_{model}}
\cdot {SS_{res} \over {df_{res}}}}\]</span></p>
<p>and</p>
<p><span class="math display">\[SS_{se}={{F_{se}\cdot df_{se}} \cdot
{SS_{res} \over {df_{res}}}}\]</span></p>
</div>
<div id="confidence-intervals" class="section level1">
<h1>Confidence intervals</h1>
<p>In option tab <code>Options</code> it is possible to ask additional
tables for the effect size indices, containing the effect size indices
and their confidence intervals (here an example with the
<code>exercise</code> dataset)</p>
<p><img src="details/glm/detail_effectsize1.png" class="img-responsive" alt=""></p>
<p><img src="details/glm/detail_effectsize2.png" class="img-responsive" alt=""></p>
<p>Details for the confidence intervals computation can be found in <a
href="https://github.com/easystats/effectsize">Ben-Shachar, Makowski
&amp; Lüdecke (2020). Compute and interpret indices of effect size.
CRAN</a></p>
<h1>
Comments?
</h1>
<p>
Got comments, issues or spotted a bug? Please open an issue on
<a href="https://github.com/gamlj/gamlj//issues "> GAMLj at github</a>
or <a href="mailto:mcfanda@gmail.com

">send me an email</a>
</p>
<script type="text/javascript" src="local.js"></script>
</div>
<div id="additional-references" class="section level1 unnumbered">
<h1 class="unnumbered">Additional references</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-cohen2014applied" class="csl-entry">
Cohen, Patricia, Stephen G West, and Leona S Aiken. 2014. <em>Applied
Multiple Regression/Correlation Analysis for the Behavioral
Sciences</em>. Psychology press.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
