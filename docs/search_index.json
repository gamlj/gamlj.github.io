[["booklet.html", "GAMLj Models Chapter 1 Introduction 1.1 Preface 1.2 Getting Started 1.3 Data 1.4 What’s in a name 1.5 General References", " GAMLj Models Marcello Gallucci 2023 Chapter 1 Introduction 1.1 Preface Draft version, mistakes may be around Here you can find a how-to about estimatin and interpreting several different types of linear model in jamovi, using the GAMLj module. This book is halfway between a software manual and a statistical how-to. So, what regards the statistical reasoning and the models interpretation may apply to analyses done also with other software, although the practical steps to obtain the results with other software may be quite different, often more difficult, as compared with GAMLj. This book, so far, covers the following models: The general linear model (2) 1.2 Getting Started First, we need to install jamovi, and within it, install GAMLj. To install jamovi, simple download it from the jamovi website. Within jamovi, access the library and install GAMLj Installing the module produces a new icon in the icons bar, and the new icon gives access to the list of the module available analyses. We are ready to go. 1.3 Data Throughout this book, we are going to use mostly two simulated datasets, containing variables that allow estimating different types of effects for different types of models. Both datasets contain two continuous independent variables, named x and z, two categorical independent variables, named cat2 and cat3, with two and three groups respectively. The dependent variable y appears in different forms or type, so we can apply to it different models. We have a continuous normally distributed dependent variable ycont, a dichotomous version of it ybin, a count version ypoi simulating a Poisson distribution of counts, a ordinal version yord with 5 ordinal levels, and ycat, a three-level categorical variable. The first dataset containing these variable is called manymodels and simulates a sample of 120 cases drawn randomly for a population. We are going to use this dataset for the general linear model and the generalized linear models. The second dataset, named clustermanymodels has the same variables, but the data a simulated as drawn from 30 different clusters. We are going to use this dataset for the mixed model and the generalized mixed models. I agree with the idea that \\(x\\), \\(y\\) and \\(z\\) datasets are boring, and real data examples are more engaging. Nonetheless, \\(x\\), \\(y\\) and \\(z\\) datasetd allow us exploring many different situations that real data often do not (at least not with one or two datasets), and do not take away the reader attention from the stats. Both datasets can be opened from jamovi data library. 1.4 What’s in a name Basically, all the fundamental analyses presented in this book are referred to as models. With the term model I mean a concise and efficient way to represent and quantify the relationships among variables. A simple regression, an ANOVA, or a multi-level random coefficients logistic regression are all models of the data. Very often these models are called statistical techniques, but I do not find this term helpful, because almost everything one does can be a technique. To avoid confusion, we call models the linear representation of the dependent variable(s) as a function of the independent variables terms. Thus, we have different models when the linear representation is different, of the estimation method is different. A logistic model, for instance, is different from a Poisson model because the first predicts the logit of a dichotomous dependent variable, the second the log of it (more on this later). In this book we cover four model categories: The general linear model (Chapter 2) The generalized linear model The mixed linear model The generalized mixed model The main differences among them, at least the ones we are interest in, can be classified depending on the type of dependent variable they model and the way the sample has been drawn: Dependent Variable Normal and Continuous Non-normal or categorical Independent casesGeneral Linear ModelGeneralized Linear Models Clustered casesMixed ModelGeneralized Mixed Models Clustered vs independent cases differ in the way the data are collected. We talk about it in Chaper (XX). In general, we will see for each model how and why one of these macro-categories applies. 1.4.1 Statistical techniques vs points of view With a model we can do many things. First, we evaluate the results. A linear model can always be evaluated from two different angles: The model fit (variances explained or deviance reduced) and the size and direction of the effects (the coefficients). Thus, whenever we estimate a model, we can look at it from each of these angles or from both. The model fit, often broken down by variables and effects unique contribution, informs us about the ability of the independent variables to explain the variability of the dependent variable. This angle offers also effect size indices, like \\(\\eta^2\\) or \\(\\omega^2\\). This angle is useful, among other things, to evaluate the strength of the effects. In the linear model, this angle is called Analysis of Variance, shorted in ANOVA, in the generalized linear model it is called Analysis of Deviance (not shorted). The second angle looks at the coefficients, that represent expected changes in the dependent variable as one compares different levels of the independent variable(s). They answer questions like “What is the average increase in salary for every year worked?”, or “what is the group with the largest salary among some employees groups?”. The coefficients angle also provides effect size indices, such as the \\(B\\) and the \\(\\beta\\) coefficients, the correlation, and several variations of standardized mean differences (Cohen’s d). This angle is useful, among other things, to evaluate both the intensity and the direction of the effects. In GAMLj, both points of view are available for all linear models handled by the module, no matter how complex they are. But this creates often confusion in users acquainted with old-fashion terminology. Why is there an ANOVA table in the results section of a regression? Why do I get regression coefficients for categorical independent variables? The reason is that in the linear models realm, terms such as regression or ANOVA are not analyses or statistical techniques, they are angles from which one looks at the results. If one is interested in the direction of the effects, one looks at the coefficients, if one is interested in the variability accounted for by the effects, one looks at the variances (or deviance). Thus, what people usually call ANOVA, meaning the analysis of a design with one continuous dependent variable and one or more categorical independent variables (factors), it is just a general linear model evaluated only from the point of view of the explained variances. What people call regression is a general linear model with continuous independent variables for which the analyst focuses on the coefficients. ANCOVA? A general linear model with at least one focal categorical variable and at least one continuous variable, for which the analyst focuses on the effect of the categorical variable knowing that the continuous variable(s) is (are) kept constant. The same applies to any other model, general, generalized, mixed or generalized mixed. In this book we keep using terms such as regression or ANOVA, keeping it in mind that we can do well without them. 1.4.2 Statistical techniques vs Analyses Alright, so what do we mean by statistical techniques in this book? Things we do with the model. If we find a main effect of a categorical independent variable, we usually want to probe it and check which group is different from any other group. We employ a posthoc test technique. Along the way, we also want to see the estimated marginal means for each level of the independent variable. If an interaction appears solid in our results, we often probe it to estimate and test the effect of one variable at different levels of another, so we do a simple effects technique. In other cases, we want to compare one big model with a smaller one, in which some terms are absent. We do a model comparison technique, so we can evaluate the overall contribution of the effects that are in the big model and not in the smaller (nested) one. GAMLj provides these techniques (and many others) for all models estimable within the module, with the same user interface and results tables. The basic principle that GAMLj tries to follow is that if we can do something in one model, we can do it with any other model. 1.4.3 Terms that we need to generalize In the methodological literature, statistical techniques emerge often in one field for one application, and then emerge again in other fields or for other applications and get different names. This may create confusion. To clarify, we are going to use some simplification. The most important ones are the following: Any ANOVA like results, not matter how they are tested (F-test, LRT, \\(\\chi^2\\)), are referred to as Omnibus tests. Moderation is an interaction in which the analyst focuses on one effect and desires to evaluated it at different levels of the other variable involved in the interaction, no matter whether the variable is continuous or categorical. A moderator is a variable that the analyst believes can change the effect of an independent variable, no matter the types of variable involved in the analysis. Estimating the effect of one variable at different levels of a moderator is called Simple Effects, no matter the types of variable involved. So, slicing of an interaction in the (classical) ANOVA or a simple slopes analysis are all refereed to as Simple Effects technique (they are indeed the same technique). Estimated marginal means are the average predicted values of a model for some level of the independent variables. No matter which model we have at hand, and what kind of variables we have, we always mean this. 1.4.4 Terms we need to cope with In linear models, independent variables are of two kinds: categorical or continuous. The former defines nominal levels (groups or conditions) the latter defines quantities. Despite the simplicity of this definition, the majority of software available calls a categorical independent variable as factor and a continuous independent variable as coviariate. jamovi and GAMLj follows this tradition. We should be aware, however, that these terms do not mean anything else that categorical and continous variable. Every independent variable in a linear model is covariated (partialed out) when the other variable effect is computed, no matter whether the variable is a categorical or continuous one. How it is covariated, however, depends on the model, so we need to pay attention to that. 1.5 General References Obviously, I did not invented any of the statistical ideas or methods that this book mentions. Much of this material comes from statistical common knowledge and logical necessity. However, when not otherwise specified, the fundamental concepts treated here can be found in the seminal work of Cohen, West, and Aiken (2014), Searle and Gruber (2016), Raudenbush and Bryk (2002), Agresti (1990), Aiken, West, and Reno (1991). When needed, specific references are provided for more novel or critical issues. References "],["model1.html", "Chapter 2 The general linear model 2.1 Introduction 2.2 GLM with one continuous independent variable 2.3 GLM with two or more continuous independent variables", " Chapter 2 The general linear model Draft version, mistakes may be around keywords General Linear Model, Regression, ANOVA, Interaction, Moderation 2.1 Introduction The general linear model (GLM) encompasses the majority of the analyses that are commonly part of any practitioner’s statistical toolbox. There are good reasons for that: with a good knowledge of the GLM we can go a long way. Within the GLM one finds analyses such as simple and multiple regression, Pearson correlation, the independent-samples t-test, the ANOVA, the ANCOVA, and many of their derivations, such as mediation analysis, planned comparisons, etc (cf. 1.4). The common theme of all these applications is that the dependent variable is a continuous variable, hopefully normally distributed, and that the sample is composed by non-related, independent cases. The most basic but yet very important application of the GLM is the simple regression. 2.2 GLM with one continuous independent variable AKA Simple Regression Consider the dataset manymodels (cf. 1.3). The dependent variable is a continuous variable named ycont, and we want to estimate its linear relation with a continuous variable named x. The extensive relation between the two variables can be appreciated in a scatterplot. It is clear that ycont and x can be any variable, as long as we can consider them as continuous. For the sake of the argument, let us imagine that we went to a bar and measured ycont as the average number of smiles smiled by each customer in a given time and \\(x\\) as the number of beers drunken for the same time span. What we want to know is what is the average increase (or decrease) of the dependent variable as the independent variable increases. Thus, how many smiles on average one should expect for one more beer. We ran a GLM to get the answer. 2.2.1 Input We set the ycont variable as the dependent variable and the x variable as the independent continuous variable (see 1.4.4), and look at the results. 2.2.2 Model Recap First, we check out the Model Info table. This is a recap table that basically says that we did what we wanted to do, and how we did it. The second table we get is the Model Fit table, were the \\(R^2\\), the adjusted \\(R^2\\), and their inferential test are presented. 2.2.3 Model Fit The \\(R^2\\) gives us the first glance of the model from the variance angle (cf. 1.4.1). The short story says that our model (in this case the independent variable \\(x\\)) explains, or account for, .323*100 percent of the variance. So, if all differences in the smiles (ycont) are set to 100, 32% of them can be associate with the amount of beers drunken (\\(x\\)). The Adj.\\(R^2\\) is the estimation of the variance explained by the model in the population, the df are the number of parameters estimated by the model (here is one because we have one independent variable that requires only one coefficient), the F is the F-test testing the null hypothesis that \\(R^2\\) is zero, and p is the probability of obtaining our results under the null hypothesis. If you find this story a bit dull, you might want to read the full story in Appendix A. 2.2.4 Omnibus Tests With only one continuous variable this table is not very useful, but we comment on it nonetheway to get you familiar with the ideas of the two points of view always available in a linear model (cf (angles)). The first line is there only for legacy compatibility purposes. It reports the inferential test of the model as a whole, that we already sow in 2.2.3. The sendon line tells us the ammount of variance of the dependent variable explained by the independent variable. In this case, the \\(p\\eta^2\\) is equal to the \\(R^2\\), bacause there is nothing to partial out (there is only one independent variable). Instructive, however, is to select the option \\(\\epsilon^2\\). we can notice that the \\(\\epsilon^2\\) is equal to the adjusted \\(R^2\\). Yes, that is going to stay: \\(R^2\\) and \\(\\eta^2\\) indices (partial or not) are the sample estimates of variance explained, whereas \\(R_{adj}^2\\) and \\(\\epsilon^2\\) effect size indices (partial or not) are the population version (\\(\\omega^2\\) is population too). People tend to use the sample version of these indices (\\(\\eta^2\\) and \\(R^2\\)) when they should actually use the population version (\\(R^2_{adj}\\) and \\(\\epsilon^2\\)). The same goes for \\(\\omega^2\\) index, but you want to read this about why you want to use them, and this how they are actually computed . In a nutshell, \\(R^2\\) and \\(\\eta^2\\) tell what happened, \\(R_{adj}^2\\) and \\(\\epsilon^2\\) tell where it came form. 2.2.5 Coefficients The coefficients table, here called the Parameters Estimates table, informs us about the size and the direction of the effect. The interesting coefficient is the one associated with the independent variable (estimate). Here it is \\(3.808\\). This means that for every unit increase in the independent variable the dependent variable increases, on average, of \\(3.808\\) units. In our toy example, for one more beer one drinks, on average, one smiles \\(3.808\\) smiles more. This is the regression coefficient, the very first and most solid pillar of the linear model building. This interpretation is going to stick, so keep it in mind, because when models get more complex, we are going to amend it, but only to make it more precise, never to twist it. The intercept, which is not focal here (nobody looks at the intercept), is worth mentioning for the sake of comparison with other software. If you run the same analysis in SPSS, R, Jasp, Stata, etc, you get the same estimate for the x variable, but a different (Intercept). Recall that in any linear model the intercept is the expected value of the dependent variable for \\(x=0\\), GAMLj, however, by default centers the independent variables to their means, so \\(x=0\\) means \\(x=\\bar{x}\\). So, in GAMLj the intercept is the expected value of the dependent variable for the average value of \\(x\\). Why centering? First, centering does not change the results of the regression coefficients of simple and multiple regression, so it is harmless in many situations. However, when an interaction is in the model, centering guarantees that the linear effects are the main effects one expects, and not some weird effects computed for the moderator equal to (possibly non existing) zero. Furthermore, I personally believe that very few variables have a real and meaningful zero, so their mean is a more sensible value than zero 1. If your variables really have a meaningful zero (which you really care about), you can always “unscale” your independent variables setting them to Original in the Covariates scaling panel. 2.2.6 Pearson Correlation Across sciences, the most used index of association between two variables is the Pearson Correlation, \\(r\\), otherwise named zero-order correlation, bivariate correlation, standardized covariance index, product-moment correlation, etc (pick any name, the Pearson correlation is a case of the Stigler law of eponymy anyway). What is important here is that the Pearson correlation is just the standardized regression coefficient of a GLM with only two continuous variables (one dep, on indep). In the GLM terminology, it takes the name of \\(\\beta\\). In our example, the correlation between ycont and x is \\(.568\\). We can verify this by asking jamovi to produce the correlation between the two variables in the Regression-&gt;Correlation Matrix menu. As expected, the correlation and the \\(\\beta\\) are the same. More specifically, the Pearson correlation is the regression coefficient that one obtains if the GLM is run after standardizing (compute the z-scores) of both dependent and independent variables. This gives us a key to interpret the Pearson correlation in a precise way: Remembering that any standardized variable has 0 mean and standard deviation equal to 1, we can interpret the \\(r\\) (and therefore the \\(\\beta\\)) as the amount of standard deviations the dependent variable moves as we move the independent variable of one standard deviation. It varies from -1 to 1, with 0 meaning no relation. When we deal with GLM’s with more than one independent variables, the link between \\(\\beata\\) and the Pearson correlation is lost, but \\(\\beta\\)’s remain the coefficients obtained after standardizing the variables, so they remain the stnadardized coefficients. 2.2.7 PLots It is always a good idea to look at your model results with a plot. A plot shows your fitted model (predicted values). Because a model is always an approximation of the real data, we want to show our predicted values against, or together, with the actual data. In Plots panel, we can set up a plot as follows: and see the results: 2.3 GLM with two or more continuous independent variables AKA Multiple Regression Let us add to our model the z variable, again a continuous variable. To keep up with our toy example, let assume that z was a measure of participants’ extroversion. The results are not update. Let go through the most important tables. 2.3.1 Model Fit The \\(R^2\\) gives us variance explained, or accounted for, of the dependent variable by the whole model. This means by both x and z, alone and together. The overall variance explained is statistically different from zero, so we can say we do explain some variance of smiling thanks to the variability of beers and extroversion. The question is now how to each independent variable contributes to this variance explained. We need to look at the individual contributions, so the Omnibus Tests. 2.3.2 Omnibus Tests Please notice that I selected \\(\\eta^2\\), partial \\(\\eta^2\\), \\(\\epsilon^2\\), and partial \\(\\epsilon^2\\), so we can interpret these indices. Before that, however, we can mention that the x variable effect is statistically different from zero, F(1,117)=58.87, p.&lt;.001, whereas the effect of z reaches the predefined level of significance by a very tiny margin, F(1,117)=4.242, p.=.042. So we can say that there is enough evidence to believe that both effects are different from zero, although the former seems more solid than the latter. Statistical significance, however, is only a part of the story: effects should be evaluated on at least three dimensions: significance, size and direction. We now want to evaluate their size. Effect size indexes are good tools to evaluate effect sizes (nomen omen). We start with the partial \\(\\eta^2\\), mainly because it is the most used and reported one effect size index in the literature (I always thought that is the case because it is the only ES produced by SPSS GLM). The partial \\(\\eta^2\\) is the proportion of variance uniquely accounted for by the independent variable, expressed as the proportion of the variance of the dependent variable not explained by the other independent variables. In short, for x (beers) is the proportion of variance of smiles not explained by “extroversion” that is explained by “beers”. In other words, it answers the question: if everybody had the same level of “extroversion”, how much variance would “beers” explain? Notice that the unique variance explained by “beers” (x), namely 31.2%, is computed after removing the variance explained by “extroversion” (z). Often you want to know how much variance a variable explains of the total variance, so of all the variance that is available. That is the \\(\\eta^2\\), the variance uniquely explained by a variable as a proportion of the total variance of the dependent variable. In other words, it answers the question: how much variance of “smiles” would “beers” uniquely explain? The \\(\\epsilon^2\\) and partial \\(\\epsilon^2\\) indexes can be interpret as the \\(\\eta\\)’s, but they are adjusted to represent the population variances, not the sample variances. So, they are better estimation of the “real” effect size indexes. 2.3.3 Household Chores Effect size indexes differences (partial vs not partial ones) are usually explained with Venn Diagrams. We try here without them. Assume you leave in a house with another person. There are 100 chores to do in your household, such us washing the dishes, cleaning the windows and take the dog out for a walk. You do 20 chores, 5 of which you did together with your companion. Your companion did 40 chores (they are always better), including the ones you did together. So, all together you people did 55 chores, your companion did 35 alone, you did 15 alone, and 5 chores were done together. You people overall contribution is 55/100, so your \\(R^2=.55\\). You alone did 15 chores, so your unique contribution is \\(\\eta^2=15/100=.15\\) of the total amount of chores to be done. However, of the chores left to do by your companion (60), you did alone 15, so your partial contribution is \\(p\\eta^2=15/60=.25\\). So the difference between \\(eta^2\\) (or any non partial ES) and its partial version is the denominator. Non partial are proportion of the total, partial ones are proportion of the total minus the part accounted for by the others. In any household we would use the \\(\\eta^2\\), but many authors are still using the \\(p\\eta^2\\). We should know what they are, and feel free to use which one one prefers. A deep discussion of this matter can be found in Olejnik and Algina (2003), that I recommend reading. 2.3.4 Coefficients Now we look at the direction and intensity of the effects. References "],["references.html", "References", " References "],["appendixa.html", "A Appendix A: The \\(R^2\\)’s A.1 Commuting \\(R^2\\) A.2 Variance explained", " A Appendix A: The \\(R^2\\)’s Contrary to what many people think, almost all effect sizes and their corresponding inferential tests in the linear models realm are based to some sort of model comparison. The \\(R^2\\) is one of them. Assume you have a dependent variable that you want to model, meaning that you want to recap it by expressing it with some predicted values. For the moment, assume the variable to be continuous. If you only have the variable scores available, without any other information, your best guess is to use the mean as the most likely value. Keeping up with our toy example, assume that ycont in the dataset was the number of smiles for a given time span done by our participants in a bar. If you only have the ycont variable, your best bet would be that the next customer will smile, on average, \\(\\hat{y}=31.7\\) times, because that is the expected value (mean) of the variable distribution. So we say, whoever comes next in the bar, they will smile \\(\\hat{y}=31.7\\) times on average. What I am saying translates in the most simple linear model, the mean model: \\[ \\hat{y_i}=\\bar{y}\\] If we use the mean as our expected value, that is our model, we have an approximation error (\\(\\sigma^2\\)), which amounts to the discrepancy between the predicted valued (the variable mean) and the actual observed values. Because errors larger or smaller than the actual values are the same, we can square the errors, and compute the average error across cases (forget about the minus 1, it is not important now). \\[ \\sigma^2_{\\bar{x}}={\\Sigma{(y_i-\\hat{y})^2} \\over {N-1}}\\] When we associate an independent variable to a dependent variable in a linear model, we are seeking a better account of the differences in the scores of the dependent variable. It is like to answering to the question “how many smiles would a person randomly taken from our sample do?”, with “it depends on how many beers they had”. If it was only for the beers, the predicted values would be \\[ \\hat{y}=a+bx\\] and the error we make would be: \\[ \\sigma^2_r={\\Sigma{[a+bx_i-y_i]^2} \\over {N-1}}\\] How good is this error variance associated with the regression? Well, it depends on how big was the error without the regression, that is using the mean as the only predictor, namely \\(\\sigma^2_{\\bar{x}}\\). So we take the difference between these possible error variances, and we know how much the regression reduced the error \\[ {\\sigma^2_{\\bar{x}}-\\sigma^2_r \\over \\sigma^2_{\\bar{x}}}=R^2\\] The \\(R^2\\) (and its variants) is the amount of error that we reduce in our predictions thanks to our model as compared to not using our model. A.1 Commuting \\(R^2\\) Let assume you commute to the university every day and it takes 60 minutes (\\(T_0\\)) to get there from your home, following one route. A friend of yours (probably a know-it-all colleague), suggests an alternative route. You follow the lead, and you got at your department in 50 minutes (\\(T_c\\)). Nice, but what was your colleague contribution to your happiness (assuming you do not enjoy commuting)? We can say that it was 10’, which is given by \\(60-50=10\\). Is that a lot? Well, it depends on the overall commuting time, because saving 10’ from Lakeville to Berwick (they are in Nova Scotia, CA, 16 minutes apart) is different than saving 10’ travelling from Milan (Italy) to Manila (Philippines), which takes around 17 hours. Thus, we can compute our colleague contribution to our happiness as: \\[ {(T_0-T_c) \\over T_0}={10 \\over60}\\] which simply means that our colleague made us save 1/6 of our journey time. This is our colleague \\(R^2\\). In statistical terms, we have the error variance without the model (\\(\\sigma_{\\bar{x}}^2\\)), the error variance of the model \\(\\sigma_{r}^2\\), and we have: \\[ R^2={\\sigma^2_{\\bar{x}}-\\sigma^2_r \\over \\sigma^2_{\\bar{x}}}\\] which is how much our model “saved” of (or reduced) our error. That is called the Proportion of Reduced Error (Judd, McClelland, and Ryan 2017). A.2 Variance explained So, why is the \\(R^2\\) index interpreted as proportion of variance explained? The reason is simply that a portion of the dependent variable variance can be associated to the variance of the independent variable(s), and thus we know why is there: because for a certain part (equal to \\(R^2\\)) people are different in the number of smiles because they are different in their number of beers. You can get a borader view on this topic consulting Judd, McClelland, and Ryan (2017). If you get exited by this, you can consult Searle and Gruber (2016), which explains that almost any test we are familiar to can be cast as a model comparison test. References "],["appendixb.html", "B Appendix B", " B Appendix B jamovi classifies data variables in four classes: Nominal : categorical factor, it is passed to the R engine as a factor. Its behavior in jamovi interface depends on the Data Type property. We have Data Type: integer it can be inserted in input field that permit numerical variable and nominal variables Data Type: text it can be inserted in input field that permit nominal variables Data Type: decimal it does not exist. Setting Data Type to decimal makes the variable a continuous type Continuous : numerical variable, it is passed to the R engine as a number. It can be input in the variable field that permit numerical variable. The data type property behaves like this: Data Type: integer it rounds the values to the closer integer Data Type: decimal allows for floating points Data Type: text it does not exist, setting Data Type to text transforms the variable into a nominal variable Ordinal : numerical variable, it is passed to the R engine as a ordered factor. It can be input in the variable field that permit numerical and ordinal variables variable. The data type property behaves like this: Data Type: integer it can be inserted in input field that permit numerical variable and nominal variables Data Type: text it can be inserted in input field that permit nominal variables Data Type: decimal it does not exist. Setting Data Type to decimal makes the variable a continuous type ID : something cool which I do not know about. "]]
