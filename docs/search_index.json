[["booklet.html", "GAMLj Models Chapter 1 Introduction 1.1 Preface 1.2 Getting Started 1.3 Data 1.4 What’s in a name 1.5 Terms we need to cope with 1.6 General References", " GAMLj Models Marcello Gallucci 2023 Chapter 1 Introduction 1.1 Preface Draft version, mistakes may be around Here you can find a how-to about estimatin and interpreting several different types of linear model in jamovi, using the GAMLj module. This book is halfway between a software manual and a statistical how-to. So, what regards the statistical reasoning and the models interpretation may apply to analyses done also with other software, although the practical steps to obtain the results with other software may be quite different, often more difficult, as compared with GAMLj. This book, so far, covers the following models: The general linear model (2) 1.2 Getting Started First, we need to install jamovi, and within it, install GAMLj. To install jamovi, simple download it from the jamovi website. Within jamovi, access the library and install GAMLj Installing the module produces a new icon in the icons bar, and the new icon gives access to the list of the module available analyses. We are ready to go. 1.3 Data Throughout this book, we are going to use mostly two simulated datasets, containing variables that allow estimating different types of effects for different types of models. Both datasets contain two continuous independent variables, named x and z, two categorical independent variables, named cat2 and cat3, with two and three groups respectively. The dependent variable y appears in different forms or type, so we can apply to it different models. We have a continuous normally distributed dependent variable ycont, a dichotomous version of it ybin, a count version ypoi simulating a Poisson distribution of counts, a ordinal version yord with 5 ordinal levels, and ycat, a three-level categorical variable. The first dataset containing these variable is called manymodels and simulates a sample of 120 cases drawn randomly for a population. We are going to use this dataset for the general linear model and the generalized linear models. The second dataset, named clustermanymodels has the same variables, but the data a simulated as drawn from 30 different clusters. We are going to use this dataset for the mixed model and the generalized mixed models. I agree with the idea that \\(x\\), \\(y\\) and \\(z\\) datasets are boring, and real data examples are more engaging. Nonetheless, \\(x\\), \\(y\\) and \\(z\\) datasetd allow us exploring many different situations that real data often do not (at least not with one or two datasets), and do not take away the reader attention from the stats. Both datasets can be opened from jamovi data library. 1.4 What’s in a name Basically, all the fundamental analyses presented in this book are referred to as models. With the term model I mean a concise and efficient way to represent and quantify the relationships among variables. A simple regression, an ANOVA, or a multi-level random coefficients logistic regression are all models of the data. Very often these models are called statistical techniques, but I do not find this term helpful, because almost everything one does can be a technique. To avoid confusion, we call models the linear representation of the dependent variable(s) as a function of the independent variables terms. Thus, we have different models when the linear representation is different, of the estimation method is different. A logistic model, for instance, is different from a Poisson model because the first predicts the logit of a dichotomous dependent variable, the second the log of it (more on this later). In this book we cover four model categories: The general linear model (Chapter 2) The generalized linear model The mixed linear model The generalized mixed model The main differences among them, at least the ones we are interest in, can be classified depending on the type of dependent variable they model and the way the sample has been drawn: Dependent Variable Normal and Continuous Non-normal or categorical Independent casesGeneral Linear ModelGeneralized Linear Models Clustered casesMixed ModelGeneralized Mixed Models Clustered vs independent cases differ in the way the data are collected. We talk about it in Chaper (XX). In general, we will see for each model how and why one of these macro-categories applies. 1.4.1 Statistical techniques vs points of view With a model we can do many things. First, we evaluate the results. A linear model can be always be evaluated from two different angles: The model fit (variances explained or deviance reduced) and the size and direction of the effects (the coefficients). Thus, whenever we estimate a model, we can look at it from each of these angles or from both. The model fit, often broken down by variables and effects unique contribution, informs us about the ability of the independent variables to explain the variability of the dependent variable. This angle offers also effect size indices, like \\(\\eta^2\\) or \\(\\omega^2\\). This angle is useful, among other things, to evaluate the strength of the effects. In the linear model, this angle is called Analysis of Variance, shorted in ANOVA, in the generalized linear model it is called Analysis of Deviance (not shorted). The second angle looks at the coefficients, that represent expected changes in the dependent variable as one compares different levels of the independent variable(s). They answer questions like “What is the average increase in salary for every year worked?”, or “what is the group with the larger salary among some employees groups?”. The coefficients angle also provides effect size indices, such as the \\(B\\) and the \\(\\beta\\) coefficients, the correlation, and several variations of standardized mean differences (Cohen’s d). This angle is useful, among other things, to evaluate both the intensity and the direction of the effects. In GAMLj, both points of view are available for all linear models handled by the module, no matter how complex they are. But this creates often confusion with users acquainted with old-fashion terminology. Why is there an ANOVA table in the results section of a regression? Why do I get regression coefficients for categorical independent variables? The reason is that in the linear models realm, terms such as regression or ANOVA are not analyses or statistical techniques, they are angles from which one looks at the results. If one is interested in the direction of the effects, one looks at the coefficients, if one is interested in the variability accounted for by the effects, one looks at the variances (or deviance). Thus, what people usually call ANOVA, meaning the analysis of a design with one continuous dependent variable and one or more categorical independent variables (factors), it is just a general linear model evaluated only from the point of view of the explained variances. What people call regression is a general linear model with continuous independent variables for which the analyst focuses on the coefficients. ANCOVA? A general linear model with at least one focal categorical variable and at least one continuous variable, for which the analyst focuses on the effect of the categorical variable knowing that the continuous variable(s) is (are) kept constant. The same applies to any other model, general, generalized, mixed or generalized mixed. In this book we keep using terms such as regression or ANOVA, keeping it in mind that we can do well without them. 1.4.2 Statistical techniques vs Analyses Alright, so what do we mean by statistical techniques in this book? Things we do with the model. If we find a main effect of a categorical independent variable, we usually want to probe it and check which group is different from any other group. We employ a posthoc test technique. Along the way, we also want to see the estimated marginal means for each level of the independent variable. If an interaction appears solid in our results, we often probe it to estimate and test the effect of one variable at different levels of another, so we do a simple effects technique. In other cases, we want to compare one big model with a smaller one, in which some terms are absent. We do a model comparison technique, so we can evaluate the overall contribution of the effects that are in the big model and not in the smaller (nested) one. GAMLj provides these techniques (and many others) for all models estimable within the module, with the same user interface and results tables. The basic principle that GAMLj tries to follow is that if we can do something in one model, we can do it with any other model. 1.4.3 Terms that we need to generalize In the methodological literature, statistical techniques emerge often in one field for one application, and then emerge again in other fields or for other applications and get different names. This may create confusion. To clarify, we are going to use some simplification. The most important ones are the following: Any ANOVA like results, not matter how they are tested (F-test, LRT, \\(\\chi^2\\)), are referred to as Omnibus tests. Moderation is an interaction in which the analyst focuses on one effect and desires to evaluated it at different levels of the other variable involved in the interaction, no matter whether the variable is continuous or categorical. A moderator is a variable that the analyst believes can change the effect of an independent variable, no matter the types of variable involved in the analysis. Estimating the effect of one variable at different levels of a moderator is called Simple Effects, no matter the types of variable involved. So, slicing of an interaction in the (classical) ANOVA or a simple slopes analysis are all refereed to as Simple Effects technique (they are indeed the same). Estimated marginal means are the average predicted values of a model for some level of the independent variables. No matter which model we have at hand, and what kind of variables we have, we always mean this. 1.5 Terms we need to cope with In linear models, independent variables are of two kinds: categorical or continuous. The former defines nominal levels (groups or conditions) the latter defines quantities. Despite the simplicity of this definition, the majority of software available calls categorical independent variable as factor and continuous independent variables as coviariate. jamovi and GAMLj follows this tradition. We should be aware, however, that these terms do not mean anything else that categorical and continous variable. Every independent variable in a linear model is covariated (partialed out) when the other variable effect is computed, no matter whether the variable is a categorical or continuous one. How it is covariated, however, depends on the model, so we need to pay attention to that. 1.6 General References Obviously, I did not invented any of the statistical ideas or methods that this book is about. Much of this material comes from statistical common knowledge and logical necessity. However, when not otherwise specified, the fundamental concepts treated here can be found in the seminal work of Cohen, West, and Aiken (2014), Searle and Gruber (2016), Raudenbush and Bryk (2002), Agresti (1990), Aiken, West, and Reno (1991). When needed, specific references are provided for more novel or critical issues. References "],["model1.html", "Chapter 2 The general linear model 2.1 Introduction 2.2 GLM with one continuous independent variable", " Chapter 2 The general linear model Draft version, mistakes may be around 2.1 Introduction The general linear model (GLM) encompasses the majority of the analyses that are commonly part of any practitioner’s statistical toolbox. There are good reasons for that: with a good knowledge of the GLM we can go a long way. Within the GLM one finds analyses such as simple and multiple regression, Pearson correlation, the paired-sample t-test, the ANOVA, the ANCOVA, and their many derivations such as mediation analysis, planned comparisons, etc (cf. 1.4). The common theme of all these applications is that the dependent variable is a continuous variable, hopefully normally distributed, and that the sample is composed by non-related, independent cases. The most basic but yet very important application of the GLM is the simple regression. 2.2 GLM with one continuous independent variable Consider the dataset manymodels (cf. 1.3). The dependent variable is a continuous variable named ycont, and we want to estimate its linear relation with a continuous variable named x. The extensive relation between the two variables can be appreciated in a scatterplot. It is clear that ycont and x can be any variable, as long as we can consider them as continuous. For the sake of the argument, let us imagine we went to a bar and measured ycont as the average number of smiles done in a given time and \\(x\\) as the number of beers drunken for that given time span. What we want to know is what is the average increase (or decrease) of the dependent variable as the independent variable increases. Thus, how many smiles on average one should expect for one more beer. We ran a GLM to get the answer. We set the ycont variable as the dependent variable and the x variable as the independent continuos variable (see 1.5), and look at the results. First, we check out the Model Info table. This is a recap table that basically says that we did what we wanted to do, and how we did it. The second table we get is the Model Fit table, were the \\(R^2\\), the adjusted \\(R^2\\), and their inferential test are presented. The \\(R^2\\) gives us the first glance of the model from the variance angle (cf. 1.4.1). The short story says that our model (in this case the independent variable \\(x\\)) explains, or account for, .323*100 percent of the variance. So, if all differences in the smiles (ycont) are set to 100, 32% of them can be associate with the amount of beers drunken (\\(x\\)). The Adj \\(R^2\\) is the estimation of the variance explained by the model in the population, the df are the number of parameters estimated by the model (here is one because we have one independent variable), the F is the F-test testing the null hypothesis that \\(R^2\\) is zero, and p is the probability of obtaining our results under the null hypothesis. If you find this story a bit dull, you might want to read the full story in Appendix A. "],["references.html", "References", " References "],["appendixa.html", "A Appendix A: The \\(R^2\\)’s A.1 Variance explained", " A Appendix A: The \\(R^2\\)’s Contrary to what many people think, almost all effect sizes and their corresponding inferential tests in the linear models realm are based to some sort of model comparison. The \\(R^2\\) is one of them. Assume you have a dependent variable that you want to model, meaning that you want to recap it by expressing it with some predicted values. For the moment, assume the variable to be continuous. If you only have the variable scores available, without any other information, your best guess is to predict the mean as the most likely value. Keeping up with our toy example, assume that ycont in the dataset was the number of smiles for a given time span done by our participants in a bar. if you only have the ycont variable, you best bet would be that the next customer will smile, on average, \\(\\hat{y}=31.7\\) times, because that is the expected value (mean) of the variable distribution. So we say, whoever comes next in the bar, they will smile \\(\\hat{y}=31.7\\) times on average. What I am saying translates in the most simple model: \\[ \\hat{y_i}=\\bar{y}\\] If we use the mean as our expected value, that is our model, we have an approximation error (\\(\\sigma^2\\)), which amounts to the discrepancy between the predicted valued (the variable mean) and the actual observed values. Because errors larger or smaller than the actual values are the same, we can square them, and compute the average error across cases (forget about the minus 1, it is not important now). \\[ \\sigma^2_{\\bar{x}}={\\Sigma{(y_i-\\hat{y})^2} \\over {N-1}}\\] When we associate an independent variable to a dependent variable in a linear model, we are seeking a better account of the differences in the scores of the dependent variable. It is like to answering to the question “how many smiles would a person randomly taken from our sample do?”, with “it depends on how many beers they had”. If it was only for the beers, the predicted values would be \\[ \\hat{y}=a+bx\\] and the error we make would be: \\[ \\sigma^2_r={\\Sigma{[(\\hat{y_i}=a+bx_i)-y_i]^2} \\over {N-1}}\\] How good is this error variance associated with the regression? Well, it depends on how big was the error without the regression, that is using the mean as the only predictor, namely \\(\\sigma^2_{\\bar{x}}\\). So we take the difference between these possible error variances, and we know how much the regression reduced the error \\[ {\\sigma^2_{\\bar{x}}-\\sigma^2_r \\over \\sigma^2_{\\bar{x}}}=R^2\\] The \\(R^2\\) (and its variants) is the amount of error that we reduce in our predictions thanks to our model as compared to not using our model. A.1 Variance explained So, why is the \\(R^2\\) index interpreted as proportion of variance explained? The reason is simply that a portion of the dependent variable variance can be associated to the variance of the independent variable(s), and thus we know why is there: because for a certain part (equal to \\(R^2\\)) people are different in the number of smailes because they are different in their number of beers. You can get a borader view on this topic consulting Judd, McClelland, and Ryan (2017). References "],["appendixb.html", "B Appendix B", " B Appendix B jamovi classifies data variables in four classes: Nominal : categorical factor, it is passed to the R engine as a factor. Its behavior in jamovi interface depends on the Data Type property. We have Data Type: integer it can be inserted in input field that permit numerical variable and nominal variables Data Type: text it can be inserted in input field that permit nominal variables Data Type: decimal it does not exist. Setting Data Type to decimal makes the variable a continuous type Continuous : numerical variable, it is passed to the R engine as a number. It can be input in the variable field that permit numerical variable. The data type property behaves like this: Data Type: integer it rounds the values to the closer integer Data Type: decimal allows for floating points Data Type: text it does not exist, setting Data Type to text transforms the variable into a nominal variable Ordinal : numerical variable, it is passed to the R engine as a ordered factor. It can be input in the variable field that permit numerical and ordinal variables variable. The data type property behaves like this: Data Type: integer it can be inserted in input field that permit numerical variable and nominal variables Data Type: text it can be inserted in input field that permit nominal variables Data Type: decimal it does not exist. Setting Data Type to decimal makes the variable a continuous type ID : something cool which I do not know about. "]]
