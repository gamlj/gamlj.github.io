# (APPENDIX) Appendix {-} 

```{r results='hide'}
library(mcdocs)

```

# Appendix A: The $R^2$'s {#appendixa}

Contrary to what many people think, almost all effect sizes and their corresponding inferential tests in the linear model's realm are based on some sort of model comparison [@judd2017data]. The $R^2$ is one of them.

Assume you have a dependent variable that you want to model, meaning that you want to recap its scores by expressing them with some predicted values. For the moment, assume the variable to be continuous. If you only have the variable scores available, without any other information, your best guess is to use the mean as the most likely value. Keeping up with our toy example, assume that `ycont` in the dataset was the number of smiles for a given time done by our participants in a bar. If you only have the `ycont` variable, your best bet would be that the next customer will smile, on average, $\hat{y}=31.7$ times, because that is the expected value (mean) of the variable distribution. So we say, whoever comes next in the bar, they will smile $\hat{y}=31.7$ times on average.

`r pic("bookletpics/ap_a_output1.png")`

What I am saying translates into the most simple linear model, the mean model:

$$ \hat{y_i}=\bar{y}$$

If we use the mean as our expected value, that is our model, we have an approximation error ($\sigma^2$), which amounts to the discrepancy between the predicted values (the variable mean) and the actual observed values. Because errors larger or smaller than the actual values are the same, we can square the errors, and compute the average error across cases (forget about the minus 1, it is not important now).

$$ \sigma^2_{\bar{x}}={\Sigma{(y_i-\hat{y})^2} \over {N-1}}$$

When we associate an independent variable with a dependent variable in a linear model, we are seeking a better account of the differences in the scores of the dependent variable. It is like answering the question "how many smiles would a  randomly taken person from our sample do?", with "it depends on how many beers they had". If it was only for the beers, the predicted values would be 

$$ \hat{y_i}=a+bx_i$$
and the error we make would be:
$$ \sigma^2_r={\Sigma{[a+bx_i-y_i]^2} \over {N-1}}$$
How good is this error variance associated with the regression? Well, it depends on how big was the error without the regression, that is using the mean as the only predictor, namely $\sigma^2_{\bar{x}}$.

So we take the difference between these possible error variances, and we know how much the regression _reduced the error_

$$ {\sigma^2_{\bar{x}}-\sigma^2_r \over \sigma^2_{\bar{x}}}=R^2$$
The $R^2$ (and its variants) is the amount of error that we reduce in our predictions thanks to our model as compared to not using our model. In other words, is the comparison of our model error with the error of a model without the independent variable(s).

## Commuting $R^2$

Let's assume you commute to the university every day and it takes 60 minutes ($T_0$) to get there from your home, following one route. A friend of yours (probably a know-it-all colleague), suggests an alternative route. You follow the lead, and you got to your department in 50 minutes ($T_c$). Nice, but what was your colleague's contribution to your happiness (assuming you do not enjoy commuting)? We can say that it was 10', which is given by $60-50=10$. Is that a lot? Well, it depends on the overall commuting time, because saving 10' from Lakeville to Berwick (they are in Nova Scotia, CA, 16 minutes apart) is different than saving 10' traveling from Milan (Italy) to Manila (Philippines), which takes around 17 hours. Thus, we can compute our colleague's contribution to our happiness as: 

$$ {(T_0-T_c) \over T_0}={10 \over60}$$

which simply means that our colleague made us save 1/6 of our journey time. This is our colleague $R^2$. In statistical terms, we have the error variance without the model ($\sigma_{\bar{x}}^2$), the error variance of the model $\sigma_{r}^2$, and we have:

$$ R^2={\sigma^2_{\bar{x}}-\sigma^2_r \over \sigma^2_{\bar{x}}}$$

which is how much our model "saved" of (or reduced) our error. That is called the _Proportion of Reduced Error_ [@judd2017data].

## Variance explained

So, why is the $R^2$ index interpreted as _proportion of variance explained_? The reason is simply that a portion of the dependent variable variance can be associated with the variance of the independent variable(s), and thus we know why is there: because for a certain part (equal to $R^2$) people are different in the number of smiles because they are different in their number of beers.

You can get a broader view of this topic by consulting @judd2017data. If you get excited by this, you can consult @searle2016linear, which explains that almost any test we are familiar with can be cast as a model comparison test.


# How many contrasts? {#a1dummies}

## Sufficiency of K-1 dummies

The fact that a linear model does not estimate all possible comparisons among levels of a categorical variable may appear puzzling. We have seen, in fact, that to represent a categorical variable with $K$ levels, we only need $K-1$ contrasts and not ($K(K-1)/2$), which will be the number of all possible comparisons. Let's see why.

A linear model requires as many coefficients as are necessary to compute the predicted values for all possible levels of the independent variables. This means that if I plug in the model a certain value of the independent variable, a sensible predicted value is produced. In simple regression $\hat{y_i}=2+3 \cdot x_i$ , for instance, if I plug $x_i=4$, I get $\hat{y_i}=14$, so it works. When you have a dichotomous independent variable, the model should work in the same way. For simplicity, assume the dichotomous IV is coded with the _dummy_ coding system, so 0 vs 1, and the groups have the same N. The model is

$$
\hat{y_i}=a+b \begin{bmatrix}
0 \\
1
\end{bmatrix}
$$
The model should be able to produce predicted values for $x=0$ and for $x=1$. It is easy to verify that this happens without problems: $\hat{y_0}=a+b \cdot 0=a$, the dependent variable mean for group 0, and  $\hat{y_1}=a+b \cdot 1=a+b$, the dependent variable mean for group 1. Now, if the IV has three groups, $x=\{1,2,3\}$, and I cast it with 2 dummies, I get this new model:

$$
\hat{y_i}=a+b_1 \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix} +b_2 \begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
$$
Is this model able to produce three predicted values? First, for all dummies equal to zero, we have the expected value for group x=1, so $a=\bar{y}_1$. Therefore $b_1=\bar{y}_2-a=\bar{y}_2-\bar{y}_1$ and $b_2=\bar{y}_3-a=\bar{y}_3-\bar{y}_1$. Thus:

* for $x=1$, the two dummies have values $0$ and $0$, so the predicted value is  $\hat{y_1}=a+b_1 \cdot 0 +b_2 \cdot 0 =\bar{y}_1$.

* for $x=2$, the two dummies have values $1$ and $0$, so the predicted value is  $\hat{y_2}=a+b_1 \cdot 1 +b_2 \cdot 0 =a+b_1=\bar{y}_1+\bar{y}_2-\bar{y}_1=\bar{y}_2$.

* for $x=3$, the two dummies have values $0$ and $1$, so the predicted value is  $\hat{y_3}=a+b_1 \cdot 0 +b_2 \cdot 1 =a+b_2=\bar{y}_1+\bar{y}_3-\bar{y}_1=\bar{y}_3$. 

No other coefficient or term is needed to account for all differences in the dependent variable due to independent variable. In fact, if a third dummy is inserted here, its coefficient must be necessarily zero, otherwise the predicted values will be biased, so the third dummy is redundant.

Often readers wonder if this works also for other coding systems. Sure it does, it is just a little more complicated to see it. Take the _deviation_ method. Here the model for three-group IV is:

$$
\hat{y_i}=a+b_1 \begin{bmatrix}
-1 \\
1 \\
0
\end{bmatrix} +b_2 \begin{bmatrix}
-1 \\
0 \\
1
\end{bmatrix}
$$

 For $x=1$, the two dummies have values $-1$ and $-1$, so the predicted value is  $\hat{y_1}=a-b_1-b_2$. For $x=2$, the two dummies have values $0$ and $1$, so the predicted value is  $\hat{y_2}=a+b_1 \cdot 1 +b_2 \cdot 0 =a+b_1$.  For $x=3$, the two dummies have values $0$ and $1$, so the predicted value is  $\hat{y_2}=a+b_1 \cdot 0 +b_2 \cdot 1 =a+b_2$. Are they the correct expected values? 

First, here the intercept $a$ is the expected mean of the dependent variable, namely $(\bar{y}_1+\bar{y}_2+\bar{y}_3)/3$, because both contrast variables are centered to 0. Therefore, $b_1=\bar{y}_2-a$ and $b_1=\bar{y}_3-a$. So:

* For $x=1$, we have $\hat{y_1}=a-b_1-b_2=3a-\bar{y}_2-\bar{y}_3=\bar{y}_1+\bar{y}_2+\bar{y}_3-\bar{y}_2+\bar{y}_3=\bar{y}_1$
* For $x=2$, we have $\hat{y_2}=a+b_1=a+\bar{y}_2-a=\bar{y}_2$
* For $x=2$, we have $\hat{y_3}=a+b_2=a+\bar{y}_3-a=\bar{y}_3$

One can show that it works for any coding system offered by `r modulename()`.

## Zero-intercept ANOVA

The demonstration in \@ref(a1dummies) may explain also a curious phenomenon, which often surprises users of the linear model (independently of the software used). If one estimates a GLM with categorical IVs without the intercept, the model consists of K coefficients, where K is the number of groups, and the coefficients values are the means of the dependent variable for the three groups. These are called zero-intercepts models. Using `manymodels` data, a GLM with `cat3` as independent, `ycont` as dependent, and no intercept, we get these coefficients:

`r pic("bookletpics/ap_a_output2.png")`

It is clear that without an intercept, the three predicted values required here can be estimated by simply recasting the model as follows:

$$
\hat{y_i}=\bar{y}_1 \begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix} +\bar{y}_2 \begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}+
\bar{y}_3 \begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}
$$

Almost any software does that automatically, including `r modulename()`. 

# Appendix Variable Types {#appendixb}

`r jamovi` classifies data variables in four classes: 

`r knitr::include_graphics("bookletpics/a_vartypes.png")`

* __Nominal__ : categorical factor, it is passed to the R engine as a factor. Its behavior in jamovi interface depends on the `Data Type` property. We have  

    * `Data Type`: `integer` can be inserted in the input field that permits numerical variable and nominal variables
    * `Data Type`: `text` can be inserted in the input field that permits nominal variables
    * `Data Type`: `decimal` it does not exist. Setting `Data Type` to `decimal` makes the variable a `continuous type`
    
* __Continuous__ : numerical variable, it is passed to the R engine as a number. It can be input in the variable field that permits numerical variable. The data type property behaves like this:

    * `Data Type`: `integer`  rounds the values to the closer integer
    * `Data Type`: `decimal` allows for floating points
    * `Data Type`: `text`  does not exist, setting `Data Type` to `text` transforms the variable into a nominal variable
    
* __Ordinal__ : numerical variable, it is passed to the R engine as an ordered factor. It can be input in the variable field that permits numerical and ordinal variables variable. The data type property behaves like this:

    * `Data Type`: `integer` can be inserted in the input field that permits numerical variable and nominal variables
    * `Data Type`: `text`  can be inserted in the input field that permits nominal variables
    * `Data Type`: `decimal`  does not exist. Setting `Data Type` to `decimal` makes the variable a `continuous type`

* __ID__ : something cool which I do not know about.

