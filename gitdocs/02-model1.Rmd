--- 
title: "GAMLj Models"
author: "Marcello Gallucci"
date: "2023"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
github-repo: mcfanda/jmvScaffold
description: "Examples of using GAMLj jamovi module to estimates different types of linear models"
---



```{r results='hide', echo=FALSE}
library(mcdocs)

```

# The general linear model {#model1}

`r draft`

`r keywords("General Linear Model, Regression, ANOVA, Interaction, Moderation")`

## Introduction

The general linear model (GLM) encompasses the majority of the analyses that are commonly part of any practitioner's statistical toolbox. There are good reasons for that: with a good knowledge of the GLM we can go a long way. Within the GLM one finds analyses such as simple and multiple regression, Pearson correlation, the independent-samples t-test, the ANOVA, the ANCOVA, and many of their derivations, such as mediation analysis, planned comparisons, etc (cf. \@ref(naming)). The common theme of all these applications is that the dependent variable is a continuous variable, hopefully normally distributed, and that the sample is composed by non-related, independent cases. The most basic but yet very important application of the GLM is the simple regression.

## GLM with one continuous independent variable

`r oldnames("Simple Regression")`

Consider the dataset `manymodels` (cf. \@ref(data)). The dependent variable is a continuous variable named `ycont`, and we want to estimate its linear relation with a continuous variable named `x`. The extensive relation between the two variables can be appreciated in a scatterplot. It is clear that `ycont` and `x` can be any variable, as long as we can consider them as continuous. For the sake of the argument, let us imagine that we went to a bar and measured `ycont` as the average number of smiles smiled by each customer in a given time and $x$ as the number of beers drunken for the same time span.

`r pic("bookletpics/2_scatterplot1.png")`

What we want to know is what is the average increase (or decrease) of the dependent variable as the independent variable increases. Thus, how many smiles on average one should expect for one more beer. We ran a GLM to get the answer.

### Input 

`r pic("bookletpics/2_menu1.png")`

We set the `ycont` variable as the dependent variable and the `x` variable as the independent continuos variable (see \@ref(covnames)), and look at the results.

`r pic("bookletpics/2_input1.png")`

### Model Recap

First, we check out the `r opt("Model Info")` table.

`r pic("bookletpics/2_output1.png")`

This is a recap table that basically says that we did what we wanted to do, and how we did it. The second table we get is the `r opt("Model Fit")` table, were the $R^2$, the adjusted $R^2$, and their inferential test are presented.

### Model Fit {#twofit}

`r pic("bookletpics/2_output2.png")`

The $R^2$ gives us the first glance of the model from the _variance angle_ (cf. \@ref(angles)). The short story says that our model (in this case the independent variable $x$) explains, or account for, .323*100 percent of the variance. So, if all differences in the smiles (`ycont`) are set to 100, 32% of them can be associate with the amount of beers drunken ($x$).  The `Adj` $R^2$ is the estimation of the variance explained by the model in the population, the `df` are the number of parameters estimated by the model (here is one because we have one independent variable that requires only one coefficient), the `F` is the F-test testing the null hypothesis that $R^2$ is zero, and `p` is the probability of obtaining our results under the null hypothesis. If you find this story a bit dull, you might want to read the full story in Appendix \@ref(appendixa).

### Omnibus Tests

`r pic("bookletpics/2_output3.png")`

With only one continuous variable this table is not very useful, but we comment on it nonetheway to get you familiar with the ideas of the two points of view always available in a linear model (cf \re@(angles)). The first line is there only for legacy compatibility purposes. It reports the inferential test of the model as a whole, that we already sow in \@ref(twofit). The sendon line tells us the ammount of variance of the dependent variable explained by the independent variable. In this case, the $p\eta^2$ is equal to the $R^2$, bacause there is nothing to partial out (there is only one independent variable). Instructive, however, is to select the option $\epsilon^2$. 

`r pic("bookletpics/2_output4.png")`

we can notice that the $\epsilon^2$ is equal to the adjusted $R^2$. Yes, that is going to stay: $R^2$ and $\eta^2$ indices (partial or not) are the sample estimates of variance explained, whereas $R_{adj}^2$ and $\epsilon^2$ effect size indices (partial or not) are the population version ($\omega^2$ is population too). People tend to use the sample version of these indices ($\eta^2$ and $R^2$) when they should actually use the population version ($R^2_{adj}$ and $\epsilon^2$). The same goes for $\omega^2$ index, but you want to `r ext_url("read this about why you want to use them","http://daniellakens.blogspot.com/2015/06/why-you-should-use-omega-squared.html")`, and `r ext_url("this how they are actually computed","https://gamlj.github.io/details_glm_effectsize.html")` . In a nutshell, $R^2$ and $\eta^2$ tell what happened, $R_{adj}^2$ and $\epsilon^2$  tell where it came form.

### Coefficients

`r pic("bookletpics/2_output5.png")`

The coefficients table, here called the _Parameters Estimates_ table, informs us about the size and the direction of the effect. The interesting coefficient is the one associated with the independent variable (`estimate`). Here it is $3.808$. This means that for every unit increase in the independent variable the dependent variable increases, on average, of $3.808$ units. In our toy example, for one more beer one drinks, on average, one smiles $3.808$ smiles more. This is the **regression coefficient**, the very first and most solid pillar of the linear model building. This interpretation is going to stick, so keep it in mind, because when models get more complex, we are going to amend it, but only to make it more precise, never to twist it. 

The intercept, which is not focal here (_nobody looks at the intercept_), is worth mentioning for the sake of comparison with other software. If you run the same analysis in SPSS, R, Jasp, Stata, etc, you get the same `estimate` for the `x` variable, but a different `(Intercept)`. Recall that in any linear model the intercept is the expected value of the dependent variable for $x=0$, `r modulename()`, however, by default centers the independent variables to their means, so $x=0$ means $x=\bar{x}$. So, in `r modulename()` the intercept is the expected value of the dependent variable for the average value of $x$. 

Why centering? First, centering does not change the results of the regression coefficients of simple and multiple regression, so it is harmless in many situations. However, when an interaction is in the model, centering guarantees that the linear effects are the _main effects_ one expects, and not some weired effects computed for the moderator equal to (possibly non existing) zero. Furthermore, I personally believe that very few variables have a real and meaningful zero, so their mean is a more sensible value than zero ^[I have nothing against zero, my favorite number is 610, which in Italian literally translates to  _you are a zero_, where "you" is really meant to  mean "we all"]. If your variables really have a meaningful zero (which you really care about), you can always "unscale" your independent variables setting them to `Original` in the `r opt("Covariates scaling")` panel.

`r pic("bookletpics/2_input2.png")`

### PLots

It is always a good idea to look at your model results with a plot. A plot shows your fitted model (predicted values). Because a model is always an approximation of the realt data, we want to show our predicted values against, or together, with the actual data. In `r opt("Plots")` panel, we can set up a plot as follows:

`r pic("bookletpics/2_input3.png")`

and see the results:

`r pic("bookletpics/2_plot1.png")`

